{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "### Resources\n",
    "[pytorch training loop](https://medium.com/data-science/the-pytorch-training-loop-3c645c56665a)\n",
    "\n",
    "### Deadlines\n",
    "**Submission deadlines:** \n",
    "**Submission deadlines:** \n",
    "- get at least **4** points by lab session (week 24-28.03.2025)\n",
    "- remaining points: by lab session (week 31.03-04.04.2025)\n",
    "\n",
    "**Points:** Aim to get 16 out of 18+ possible points\n",
    "\n",
    "## Submission instructions\n",
    "The class is held on-site in lab rooms. Please prepare you notebook on your computer or anywhere in the cloud (try using DeepNote or Google Colab).\n",
    "\n",
    "Make sure you know all the questions and answers, and that the notebook contains results; before presentation do `Runtime -> Restart and run all`\n",
    "\n",
    "![Alt text](data:image/png;base64,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)\n",
    "\n",
    "We provide starter code, however you are not required to use it as long as you properly solve the tasks.\n",
    "\n",
    "## Extra points\n",
    "\n",
    "You can earn extra 2 points if all your experiments are logged with [Weights and Biases](http://wandb.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1 [2p]:\n",
    "\n",
    "Let's see why GPUs are useful in deep learning. Compare matrix multiplication speed for a few matrix shapes when implemented:\n",
    "1. as loops in Python\n",
    "2. using np.einsum\n",
    "3. using numpy on CPU\n",
    "4. using pytorch on CPU\n",
    "5. using pytorch on GPU\n",
    "\n",
    "Finally, consider two square matrices, $A$ and $B$. We have 4 possibilities of multiplying them or their transpositions:\n",
    "1. $AB$\n",
    "2. $A^TB$\n",
    "3. $AB^T$\n",
    "4. $A^TB^T$\n",
    "\n",
    "Which option is the fastest? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Stochastic Gradient Descent (training MNIST digits) [2p]\n",
    "\n",
    "We provide below starter code that trains a classification model (with softmax + cross entropy loss). Alternatively, implement your own training loop and use it to solve this problem jointly with the next one.\n",
    "\n",
    "Implement the following additions to the SGD code provided:\n",
    "  1. **[1p]** momentum\n",
    "  2. **[0.5p]** learning rate schedule\n",
    "  3. **[0.5p]** weight decay, in which we additionally minimize for each weight matrix (but typically not the bias) the sum of its elements squared. One way to implement it is to use the function `model.named_parameters` and select all parameters whose names contain \"`weight`\" rather than \"`bias`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Tuning the Network for MNIST [2p]\n",
    "\n",
    "Tune the following network to reach **validation error rate below 1.9%**.\n",
    "This should result in a **test error rate below 2%**. To\n",
    "tune the network you will need to:\n",
    "1. Choose the number of layers (more than 1, less than 5);\n",
    "2. Choose the number of neurons in each layer (more than 100,\n",
    "    less than 5000);\n",
    "3. Pick proper weight initialization;\n",
    "4. Pick proper learning rate schedule (need to decay over time,\n",
    "    a good range to check on MNIST is about 1e-2 ... 1e-1 at the beginning and\n",
    "    half of that after 10000 batches);\n",
    "5. Pick a momentum constant (probably a constant one will be OK).\n",
    "\n",
    "\n",
    "Please note: there are many hyperparameter settings that give the desired answer, some may require tuning all hyperparameters, some only a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Convolutional Network [2p]\n",
    "\n",
    "Use convolutional and max-pooling layers (`Conv2d`, `Max_pool2d` or their functional variants) and (without dropout) get a test error rate below 1.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Data Augmentation [1p]\n",
    "\n",
    "Apply data augmentation methods (e.g. rotations, noise, crops) when training networks on MNIST, to significantly reduce test error rate for your network. You can use functions from the [torchvision.transforms](http://pytorch.org/docs/master/torchvision/transforms.html) module.\n",
    "\n",
    "Please note: when using random transformations during training, make sure they are re-computed in every epoch. Consider applying augmentation either in the training loop or in the `InMemDataLoader`. For the second case, function `InMemDataLoader.__iter__` is a good place to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 6: Dropout [2p]\n",
    "\n",
    "Learn about dropout:\n",
    "\n",
    "- implement a **dropout** layer \n",
    "- or use `nn.Dropout` (then the exercise is worth 1.5 points)\n",
    "\n",
    "and try to train a\n",
    "network getting below 1.5% test error rates with dropout, but no convolutions, or below 1% when dropout is used jointly with convolutions!\n",
    "\n",
    "Remember to turn off dropout during testing, using `model.train()` and `model.eval()`!\n",
    "\n",
    "Hint: Use [torch.nn.functional.dropout](http://pytorch.org/docs/master/nn.html#torch.nn.functional.dropout).\n",
    "\n",
    "Details: http://arxiv.org/pdf/1207.0580.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Batch Normalization [2p]\n",
    "\n",
    "[Batch Normalization](https://arxiv.org/abs/1502.03167) helps training neural networks because it [normalizes layer activation magnitudes](https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf). It typically allows to train networks faster and/or with higher learning rates, lessens the importance\n",
    "of initialization and might eliminate the need for Dropout.\n",
    "\n",
    "Implement Batch Normalization and compare with regular training of MNIST models.\n",
    "\n",
    "Remember to use the batch statistics during model training and to use an average of training batch statistics during model evaluation. For details please consult the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8: Norm Constraints [1p]\n",
    "\n",
    "Implement norm constraints, i.e. instead of weight decay, that tries to set all weights to small values, apply a limit on the total\n",
    "norm of connections incoming to a neuron. In our case, this\n",
    "corresponds to clipping the norm of *rows* of weight\n",
    "matrices. An easy way of implementing it is to make a gradient\n",
    "step, then look at the norm of rows and scale down those that are\n",
    "over the threshold (this technique is called \"projected gradient descent\").\n",
    "\n",
    "Please consult the Dropout paper (http://arxiv.org/pdf/1207.0580.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9: Hyperparameter tuner [2p]\n",
    "\n",
    "Implement a hyper-parameter tuner able to optimize the learning rate schedule, number of neurons, and similar hyperparameters. To start, use a random search (please see http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf and especially Fig 1. for intuitions on why random search is better than grid search). It may be a good idea to use a fixed maximum number of epochs (or training time) for each optimization trial to prevent selecting hyperparameters that yield slowly converging solutions. A good result will be a set of hyperparameters that reach on MNIST solutions with test errors less than $1.3\\%$ in no more than 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10: Pruning [1p]\n",
    "\n",
    "Prune the MNIST network to retain validation accuracy no worse than 0.1 percentage point at maximum sparsity (maximal number of weights removed from the network).\n",
    "\n",
    "One way to do it is to \n",
    "1. train the network, \n",
    "2. set to zero the smallest weights (typically you can zero up to 50% of weights)\n",
    "3. retrain the network, keeping the zeroed weights zeroed, and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11: Other tricks [1p-many]\n",
    "\n",
    "The neural network literature is full of tricks for training neural networks. Find some and implement them. Please note: the number of points depends on the hardness of the extension you want to implement. If in doubt, consult the TA beforehand"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
