{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e20978d",
   "metadata": {
    "id": "2e20978d"
   },
   "source": [
    "# Assignment 5\n",
    "\n",
    "**Submission deadlines:**\n",
    "\n",
    "  - Tuesday/Wednesday/Friday groups: 27-30.05.2025\n",
    "\n",
    "**Points:** Aim to get 15 out of 19 points.\n",
    "\n",
    "EDIT (2025.05.13): \n",
    "\n",
    "* Task 2. Added link to Wiki links.\n",
    "* Task 3. Added link to data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09933a68",
   "metadata": {
    "id": "09933a68"
   },
   "source": [
    "## Task 1 [3p]\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left; see the [data file](https://drive.google.com/file/d/1WoBT5OrTlNnjHg6jN90RnRb7krpEzFsP/view?usp=drive_link). The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss with negative sampling (NS) as in [Mikolov et al. 2013](https://arxiv.org/pdf/1310.4546) (see section 2.2). The loss function is as follows:\n",
    "$$\n",
    " L = -\\log \\sigma(u_o^T v_c) - \\sum_{i=1}^k \\log \\sigma(-u_i^T v_c);\n",
    "$$\n",
    "see [SKOS info](https://skos.ii.uni.wroc.pl/course/view.php?id=738#section-10) for more details.\n",
    "\n",
    "\n",
    "Compute the gradient manually. You can use some gradient clipping, or regularizaton.\n",
    "\n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f744e4",
   "metadata": {},
   "source": [
    "## Task 2 [2p*]\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file](https://drive.google.com/file/d/1H0ChgZjcbW7x3Gy_9RK0CoduP5M8WscP/view?usp=drive_link) containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8781f2",
   "metadata": {},
   "source": [
    "# Task 3 [3p]\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences); see files [polish_lower](https://drive.google.com/file/d/1H1WMHpf3UXe2Q_QrhuvbxPTbsinvHIHr/view?usp=drive_link), [polish_upper](https://drive.google.com/file/d/1H5117jA6VKiGEizqu15ap6eWkDdbE02t/view?usp=drive_link) . Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n",
    "\n",
    "Present embedding using the PCA method or t-SNE. You can use sklearn or any other library for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a36bf",
   "metadata": {},
   "source": [
    "# Task 4: Word Embeddings [4p]\n",
    "\n",
    "Many natural language processing tasks requrie continuous representations for words.\n",
    "[Word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are mappings from a discrete\n",
    "space to real-valued vectors. Word embeddings might be trained with neural networks,\n",
    "either as a by-product of other tasks (e.g., language modeling, neural machine translation),\n",
    "or with networks designed specifically for the word embedding task.\n",
    "\n",
    "Two problems associated with training neural word embeddings are related to the speed of training:\n",
    "(a) large volume of data, on which the network has to be trained on, and (b) time required to compute\n",
    "output probability distribution over large vocabularities.\n",
    "\n",
    "One of the most popular architectures for training word embeddings is called Word2vec [[1]()], [[2]()]. It builds on the idea that semantics of a word can be defined through the contexts,\n",
    "in which the word appears in the sentence.\n",
    "\n",
    "Let $w_1, w_2,\\ldots,w_N$ be an $N$-word sentence in a natural language.\n",
    "We define a context of a word $w_l$ a list of $n$ preceeding and following words\n",
    "$w_{l-n},\\ldots,w_{l-1},w_{l+1},\\dots,w_{l+n}$.\n",
    "\n",
    "The underlying assumption is that similar words appear in similar contexts.\n",
    "For instance, words *Poland* and *Monaco* are similar in a sense, that they are singular nouns\n",
    "describing abstract concepts of existing, european countries.\n",
    "We can convince ourselves by looking at exceprts from Wikipedia articles\n",
    "on Poland and Monaco:\n",
    "\n",
    "* Despite **Monaco's independence** and separate foreign policy\n",
    "* aimed to preserve **Poland's independence** and the szlachta's\n",
    "\n",
    "* **Monaco joined the** Council of Europe in 2004\n",
    "* **Poland joined the** Schengen Area in 2007\n",
    "\n",
    "* nearly one-fifth **of Poland's population** – half of them\n",
    "* Christians comprise a total of 83.2% **of Monaco's population**.\n",
    "\n",
    "### Tasks\n",
    "You will use word vectors pre-computed on a large dataset.\n",
    "1. **[1p]** It has been observed, that word embeddings allow to perform semantic arithmetic where, for instance\n",
    "\n",
    "    **king** - **man** + **woman** ~= **queen**\n",
    "\n",
    "    This *analogy* task is often used as a quality measure of word embeddings. Load word embeddings and compute\n",
    "    their analogy score on a dataset of analogous pairs, expressed as an accuracy of accuracy of predicting a pair\n",
    "    item (**queen** in the example above). Specifically, compare `FastText` and `Word2vec` word embeddings.\n",
    "    \n",
    "2. **[1p]** Word embedding capture approximate semantics. Under an assumption that words of similar semantics\n",
    "    exist in different languages, a mapping $W: \\mathbb{R}^{300}\\mapsto\\mathbb{R}^{300}$ might be constructed that\n",
    "    translates word embeddings between languages. It has been shown that such ortonormal mappings allow to express\n",
    "    approximate, bilingual dictionaries. In addition, non-linear mappings do not offer additional benefits.\n",
    "\n",
    "    Given a simple English-Polish dictionary of word pairs (sourced from Wikitionary)\n",
    "    find an orthonormal mapping $W$ between English and Polish `FastText`\n",
    "    word embeddings using Procrustes analysis.\n",
    "\n",
    "3. **[1p]** Word embeddings can often be nicely visualized.\n",
    "    Make a 2-D `PCA` plot of word embeddings for countries and their capital cities\n",
    "    for `FastText` or `Word2vec`. Connect each country with its capital city with a line segment.\n",
    "    Can you see any regularities?\n",
    "    \n",
    "4. **[1p]** Plot 400 roughly most frequent words' embeddings (either `FastText` or `Word2vec`) in 2-D with `PCA`.\n",
    "    Skip stop words, punctuations, artifact words, etc. You can be imprecise and use heuristics\n",
    "    (e.g., select words than are at lest 3 charactes long).\n",
    "    Can you see any regularities? Another method of making meaningful visualizations is `t-SNE`.\n",
    "    \n",
    "    Make another 2-D visualization, this time using `t-SNE`. Visualizations with `t-SNE` are obtained\n",
    "    with gradient descent. Try to tweak optimization parameters to get lower optimization error,\n",
    "    than the one with default parameters.\n",
    "    Can you see any regularities this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f3460",
   "metadata": {},
   "source": [
    "# Task 5: Language Model [5p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc60ed1",
   "metadata": {},
   "source": [
    "Build a basic language model using a publicly available text dataset. You'll experiment with RNN-based architectures (Simple RNN, LSTM, GRU) to learn how they model sequences.\n",
    "\n",
    "### **Part 1: Dataset Download & Preparation (1 point)**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Download a publicly available dataset, e.g., *Alice’s Adventures in Wonderland* from Project Gutenberg.\n",
    "  * Use requests or a dataset API like torchtext.datasets.\n",
    "* Preprocess the text:\n",
    "  * Lowercase, remove non-alphabetic characters.\n",
    "  * Tokenize into words (use nltk or spaCy).\n",
    "  * Build a vocabulary, keeping frequent words (e.g., top 10k).\n",
    "* Use **pretrained word embeddings** (e.g., GloVe 100d or FastText):\n",
    "  * Load with torchtext.vocab, gensim, or similar.\n",
    "  * Initialize the embedding layer with pretrained vectors.\n",
    "\n",
    "\n",
    "### **Part 2: Build a Recurrent Language Model (1 point)**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Implement a word-level language model using:\n",
    "  * Pretrained embedding layer (frozen or trainable).\n",
    "  * A single-layer **Simple RNN**.\n",
    "  * A fully connected output layer with softmax.\n",
    "\n",
    "### **Part 3: Train the Model (1 point)**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Use cross-entropy loss.\n",
    "* Predict the next word from a sequence.\n",
    "* Use teacher forcing and batching.\n",
    "* Plot training loss over time.\n",
    "\n",
    "### **Part 4: Generate Text (1 point)**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Given a seed sequence, generate text of specified length.\n",
    "* Use **temperature sampling** to vary creativity.\n",
    "* Try different temperatures and compare.\n",
    "\n",
    "### **Part 5: Evaluation & Reflection (1 point) -> W&B report**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Evaluate model outputs: does it learn sentence structure?\n",
    "* Reflect on limitations of the Simple RNN and its behavior on longer sequences.\n",
    "\n",
    "### **Bonus Section (Up to +2 Points): Model Comparison**\n",
    "\n",
    "Compare the performance of three models:\n",
    "\n",
    "\n",
    "1. Simple RNN\n",
    "2. LSTM\n",
    "3. GRU\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Implement the same model architecture but switch out the recurrent layer.\n",
    "* Train all three models under the same conditions.\n",
    "* Record and compare:\n",
    "  * Training time\n",
    "  * Final loss\n",
    "  * Generated text quality\n",
    "* (Optional) Add dropout to recurrent layers and observe effects.\n",
    "* Summarize findings in a table or chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7644d36",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
