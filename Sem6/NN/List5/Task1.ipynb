{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e756312",
   "metadata": {},
   "source": [
    "## Task 1 [3p]\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left; see the [data file](https://drive.google.com/file/d/1WoBT5OrTlNnjHg6jN90RnRb7krpEzFsP/view?usp=drive_link). The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss with negative sampling (NS) as in [Mikolov et al. 2013](https://arxiv.org/pdf/1310.4546) (see section 2.2). The loss function is as follows:  \n",
    "Given:  \n",
    "- A **center word** w_c\n",
    "A **true context word** w_o\n",
    "k **negative samples**: words not in the context (denoted as w_1, ..., w_k)\n",
    "Word vectors:\n",
    "- v_c = embedding of the center word (from input matrix)\n",
    "- u_o = embedding of the context word (from output matrix)\n",
    "- u_i = embeddings of negative samples\n",
    "\n",
    "$$\n",
    " L = -\\log \\sigma(u_o^T v_c) - \\sum_{i=1}^k \\log \\sigma(-u_i^T v_c);\n",
    "$$\n",
    "\n",
    "(see [SKOS info](https://skos.ii.uni.wroc.pl/course/view.php?id=738#section-10) for more details)\n",
    "\n",
    "\n",
    "Compute the gradient manually. You can use some gradient clipping, or regularizaton.\n",
    "\n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82895561",
   "metadata": {},
   "source": [
    "u - object vector, v - context vector  \n",
    "o - object, c - conteext   \n",
    "$P(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w\\in V} \\exp(u_w^T v_c)}$\n",
    "\n",
    "**Negative Sampling:**  \n",
    "$$J_{neg-sample}(u_o, v_c, u_i) = -\\log \\sigma(u_o^T v_c) - \\sum_{k \\in \\{\\text{K sampled indices}\\}} \\log \\sigma(-u_i^T v_c)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b4753",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a47585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218137fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    pairs = []\n",
    "    vocab = set()\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            # safety reasons (should not happen)\n",
    "            if len(parts) < 2: continue\n",
    "            center = parts[0]\n",
    "            context = parts[-1]\n",
    "            pairs.append((center, context))\n",
    "            vocab.update([center, context])\n",
    "    return pairs, sorted(vocab)\n",
    "\n",
    "def build_vocab(vocab):\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02929381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def get_negative_samples(context_idx, vocab_size, k):\n",
    "    negatives = []\n",
    "    while len(negatives) < k:\n",
    "        neg = random.randint(0, vocab_size - 1)\n",
    "        if neg != context_idx:\n",
    "            negatives.append(neg)\n",
    "    return negatives\n",
    "\n",
    "def initialize_embeddings(vocab_size, embed_dim):\n",
    "    input_vectors = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "    output_vectors = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "    return input_vectors, output_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(pairs, word2idx, embed_dim=50, epochs=5, lr=0.05, k=5):\n",
    "    vocab_size = len(word2idx)\n",
    "    input_vectors, output_vectors = initialize_embeddings(vocab_size, embed_dim)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        random.shuffle(pairs)\n",
    "        pbar = tqdm(pairs, desc=f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "        for center, context in pbar:\n",
    "            center_idx = word2idx[center]\n",
    "            context_idx = word2idx[context]\n",
    "\n",
    "            v_c = input_vectors[center_idx]\n",
    "            u_o = output_vectors[context_idx]\n",
    "\n",
    "            # positive sample\n",
    "            z = np.dot(u_o, v_c)\n",
    "            p = sigmoid(z)\n",
    "            grad_vc = (p - 1) * u_o\n",
    "            grad_uo = (p - 1) * v_c\n",
    "\n",
    "            input_vectors[center_idx] -= lr * grad_vc\n",
    "            output_vectors[context_idx] -= lr * grad_uo\n",
    "\n",
    "            # negative samples\n",
    "            neg_indices = get_negative_samples(context_idx, vocab_size, k)\n",
    "            for neg_idx in neg_indices:\n",
    "                u_k = output_vectors[neg_idx]\n",
    "                z_neg = np.dot(u_k, v_c)\n",
    "                p_neg = sigmoid(-z_neg)\n",
    "                grad_vc += (1 - p_neg) * u_k\n",
    "                grad_uk = (1 - p_neg) * v_c\n",
    "\n",
    "                output_vectors[neg_idx] -= lr * grad_uk\n",
    "\n",
    "            input_vectors[center_idx] -= lr * grad_vc\n",
    "\n",
    "            loss = -np.log(p + 1e-8) - sum(np.log(sigmoid(-np.dot(output_vectors[neg], v_c)) + 1e-8) for neg in neg_indices)\n",
    "            total_loss += loss\n",
    "            pbar.set_postfix(loss=total_loss / (1 + pbar.n))\n",
    "\n",
    "    return input_vectors, word2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5ae53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(filename, input_vectors, word2idx):\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    with open(filename, 'w') as f:\n",
    "        for idx, vector in enumerate(input_vectors):\n",
    "            word = idx2word[idx]\n",
    "            vec_str = ' '.join(map(str, vector))\n",
    "            f.write(f\"{word} {vec_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efe76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e3d95f07ce4f159b3848a5d1425bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5525116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372a21cd439a409a82bf1bfbaf9635c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/5525116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_file = 'task1_objects_contexts_polish.txt'\n",
    "pairs, vocab = read_data(data_file)\n",
    "word2idx, _ = build_vocab(vocab)\n",
    "\n",
    "input_vectors, word2idx = train_word2vec(pairs, word2idx, embed_dim=10, epochs=3, lr=0.05, k=5)\n",
    "\n",
    "save_vectors('vectors.txt', input_vectors, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956dacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings(vectors, word2idx, num_points=100):\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(vectors[:num_points])\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i in range(num_points):\n",
    "        plt.scatter(reduced[i, 0], reduced[i, 1], alpha=0.6)\n",
    "        plt.text(reduced[i, 0], reduced[i, 1], idx2word[i], fontsize=9)\n",
    "    plt.title(\"PCA of Word Embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb25b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(input_vectors, word2idx, num_points=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
