{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e756312",
   "metadata": {},
   "source": [
    "## Task 1 [3p]\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left; see the [data file](https://drive.google.com/file/d/1WoBT5OrTlNnjHg6jN90RnRb7krpEzFsP/view?usp=drive_link). The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss with negative sampling (NS) as in [Mikolov et al. 2013](https://arxiv.org/pdf/1310.4546) (see section 2.2). The loss function is as follows:  \n",
    "Given:  \n",
    "- A **center word** w_c\n",
    "A **true context word** w_o\n",
    "k **negative samples**: words not in the context (denoted as w_1, ..., w_k)\n",
    "Word vectors:\n",
    "- v_c = embedding of the center word (from input matrix)\n",
    "- u_o = embedding of the context word (from output matrix)\n",
    "- u_i = embeddings of negative samples\n",
    "\n",
    "$$\n",
    " L = -\\log \\sigma(u_o^T v_c) - \\sum_{i=1}^k \\log \\sigma(-u_i^T v_c);\n",
    "$$\n",
    "\n",
    "(see [SKOS info](https://skos.ii.uni.wroc.pl/course/view.php?id=738#section-10) for more details)\n",
    "\n",
    "\n",
    "Compute the gradient manually. You can use some gradient clipping, or regularizaton.\n",
    "\n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82895561",
   "metadata": {},
   "source": [
    "u - object vector, v - context vector  \n",
    "o - object, c - conteext   \n",
    "$P(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w\\in V} \\exp(u_w^T v_c)}$\n",
    "\n",
    "**Negative Sampling:**  \n",
    "$$J_{neg-sample}(u_o, v_c, u_i) = -\\log \\sigma(u_o^T v_c) - \\sum_{k \\in \\{\\text{K sampled indices}\\}} \\log \\sigma(-u_i^T v_c)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e655554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90db385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # 100-300\n",
    "epochs = 3   # 3-6\n",
    "learning_rate = 0.003\n",
    "lr_decay = 0.0001  # linear lr decay\n",
    "neg_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task1_objects_contexts_polish.txt', 'r') as f:\n",
    "    pairs = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "# use part of data for training\n",
    "N_pairs = len(pairs)\n",
    "pairs = np.random.permutation(pairs)\n",
    "pairs = pairs[:int(0.01 * N_pairs)]\n",
    "print(f\"Using {len(pairs)} pairs for training.\")\n",
    "\n",
    "# generate vocab of all words, separated when they are center and context\n",
    "objects = set([obj for obj, ctx in pairs])\n",
    "contexts = set([ctx for obj, ctx in pairs])\n",
    "print(f\"{len(objects)} unique objects, {len(contexts)} unique contexts\")\n",
    "\n",
    "# index words\n",
    "object_idx = {w: i for i, w in enumerate(objects)}\n",
    "idx_object = {i: w for i, w in enumerate(objects)}\n",
    "\n",
    "context_idx = {w: i for i, w in enumerate(contexts)}\n",
    "idx_context = {i: w for i, w in enumerate(contexts)}\n",
    "\n",
    "\n",
    "# create vector embedings\n",
    "embed_center = [[0]*embedding_dim]*len(objects) # v_w when w is center\n",
    "embed_context = [[0]*embedding_dim]*len(objects) # v_w when w is center\n",
    "\n",
    "\n",
    "# count number of word occurences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1da48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4109/5525116 [01:12<26:56:18, 56.93it/s]\n",
      "  0%|          | 0/10 [01:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m obj_idx \u001b[38;5;241m=\u001b[39m word2idx[obj]\n\u001b[0;32m     44\u001b[0m ctx_idx \u001b[38;5;241m=\u001b[39m word2idx[ctx]\n\u001b[1;32m---> 45\u001b[0m neg_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_negative_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m v_c \u001b[38;5;241m=\u001b[39m V[ctx_idx]\n\u001b[0;32m     48\u001b[0m u_o \u001b[38;5;241m=\u001b[39m U[obj_idx]\n",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m, in \u001b[0;36mget_negative_samples\u001b[1;34m(pos_idx, k)\u001b[0m\n\u001b[0;32m     33\u001b[0m negs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(negs) \u001b[38;5;241m<\u001b[39m k:\n\u001b[1;32m---> 35\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneg_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;241m!=\u001b[39m pos_idx:\n\u001b[0;32m     37\u001b[0m         negs\u001b[38;5;241m.\u001b[39mappend(sample)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('task1_objects_contexts_polish.txt', 'r') as f:\n",
    "    pairs = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "objects = sorted(set([obj for obj, ctx in pairs]))\n",
    "contexts = sorted(set([ctx for obj, ctx in pairs]))\n",
    "vocab = sorted(set(objects + contexts))\n",
    "\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "epochs = 10\n",
    "learning_rate = 0.05\n",
    "neg_samples = 5\n",
    "\n",
    "# Initialize embeddings\n",
    "U = np.random.randn(len(vocab), embedding_dim) * 0.01  # object vectors\n",
    "V = np.random.randn(len(vocab), embedding_dim) * 0.01  # context vectors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# build distribution for negative sampling\n",
    "word_freq = Counter([ctx for obj, ctx in pairs])\n",
    "freqs = np.array([word_freq.get(w, 1) for w in vocab])\n",
    "neg_dist = freqs ** 0.75\n",
    "neg_dist /= neg_dist.sum()   # normalize to 1\n",
    "\n",
    "def get_negative_samples(pos_idx, k):\n",
    "    negs = []\n",
    "    while len(negs) < k:\n",
    "        sample = np.random.choice(len(vocab), p=neg_dist)\n",
    "        if sample != pos_idx:\n",
    "            negs.append(sample)\n",
    "    return negs\n",
    "\n",
    "for epoch in tqdm(range(epochs), position=0):\n",
    "    # np.random.shuffle(pairs)\n",
    "    for obj, ctx in tqdm(pairs, position=1):\n",
    "        obj_idx = word2idx[obj]\n",
    "        ctx_idx = word2idx[ctx]\n",
    "        neg_indices = get_negative_samples(ctx_idx, neg_samples)\n",
    "\n",
    "        v_c = V[ctx_idx]\n",
    "        u_o = U[obj_idx]\n",
    "        u_negs = U[neg_indices]\n",
    "\n",
    "        # Positive sample\n",
    "        score_pos = np.dot(u_o, v_c)\n",
    "        grad_pos = sigmoid(-score_pos)\n",
    "\n",
    "        # Negative samples\n",
    "        score_negs = np.dot(u_negs, v_c)\n",
    "        grad_negs = sigmoid(score_negs)\n",
    "\n",
    "        # Gradients\n",
    "        grad_u_o = grad_pos * v_c\n",
    "        grad_v_c = grad_pos * u_o - np.sum(grad_negs[:, None] * u_negs, axis=0)\n",
    "        grad_u_negs = grad_negs[:, None] * v_c\n",
    "\n",
    "        # Update\n",
    "        U[obj_idx] -= learning_rate * grad_u_o\n",
    "        V[ctx_idx] -= learning_rate * grad_v_c\n",
    "        U[neg_indices] -= learning_rate * grad_u_negs\n",
    "\n",
    "# Save object vectors\n",
    "with open('object_vectors.txt', 'w') as f:\n",
    "    for obj in objects:\n",
    "        idx = word2idx[obj]\n",
    "        vec = U[idx]\n",
    "        f.write(f\"{obj} {' '.join(map(str, vec))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
