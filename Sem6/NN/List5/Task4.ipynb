{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c59bd4d",
   "metadata": {},
   "source": [
    "# Task 4: Word Embeddings [4p]\n",
    "\n",
    "Many natural language processing tasks requrie continuous representations for words.\n",
    "[Word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are mappings from a discrete\n",
    "space to real-valued vectors. Word embeddings might be trained with neural networks,\n",
    "either as a by-product of other tasks (e.g., language modeling, neural machine translation),\n",
    "or with networks designed specifically for the word embedding task.\n",
    "\n",
    "Two problems associated with training neural word embeddings are related to the speed of training:\n",
    "(a) large volume of data, on which the network has to be trained on, and (b) time required to compute\n",
    "output probability distribution over large vocabularities.\n",
    "\n",
    "One of the most popular architectures for training word embeddings is called Word2vec [[1]()], [[2]()]. It builds on the idea that semantics of a word can be defined through the contexts,\n",
    "in which the word appears in the sentence.\n",
    "\n",
    "Let $w_1, w_2,\\ldots,w_N$ be an $N$-word sentence in a natural language.\n",
    "We define a context of a word $w_l$ a list of $n$ preceeding and following words\n",
    "$w_{l-n},\\ldots,w_{l-1},w_{l+1},\\dots,w_{l+n}$.\n",
    "\n",
    "The underlying assumption is that similar words appear in similar contexts.\n",
    "For instance, words *Poland* and *Monaco* are similar in a sense, that they are singular nouns\n",
    "describing abstract concepts of existing, european countries.\n",
    "We can convince ourselves by looking at exceprts from Wikipedia articles\n",
    "on Poland and Monaco:\n",
    "\n",
    "* Despite **Monaco's independence** and separate foreign policy\n",
    "* aimed to preserve **Poland's independence** and the szlachta's\n",
    "\n",
    "* **Monaco joined the** Council of Europe in 2004\n",
    "* **Poland joined the** Schengen Area in 2007\n",
    "\n",
    "* nearly one-fifth **of Poland's population** â€“ half of them\n",
    "* Christians comprise a total of 83.2% **of Monaco's population**.\n",
    "\n",
    "### Tasks\n",
    "You will use word vectors pre-computed on a large dataset.\n",
    "1. **[1p]** It has been observed, that word embeddings allow to perform semantic arithmetic where, for instance\n",
    "\n",
    "    **king** - **man** + **woman** ~= **queen**\n",
    "\n",
    "    This *analogy* task is often used as a quality measure of word embeddings. Load word embeddings and compute\n",
    "    their analogy score on a dataset of analogous pairs, expressed as an accuracy of accuracy of predicting a pair\n",
    "    item (**queen** in the example above). Specifically, compare `FastText` and `Word2vec` word embeddings.\n",
    "    \n",
    "2. **[1p]** Word embedding capture approximate semantics. Under an assumption that words of similar semantics\n",
    "    exist in different languages, a mapping $W: \\mathbb{R}^{300}\\mapsto\\mathbb{R}^{300}$ might be constructed that\n",
    "    translates word embeddings between languages. It has been shown that such ortonormal mappings allow to express\n",
    "    approximate, bilingual dictionaries. In addition, non-linear mappings do not offer additional benefits.\n",
    "\n",
    "    Given a simple English-Polish dictionary of word pairs (sourced from Wikitionary)\n",
    "    find an orthonormal mapping $W$ between English and Polish `FastText`\n",
    "    word embeddings using Procrustes analysis.\n",
    "\n",
    "3. **[1p]** Word embeddings can often be nicely visualized.\n",
    "    Make a 2-D `PCA` plot of word embeddings for countries and their capital cities\n",
    "    for `FastText` or `Word2vec`. Connect each country with its capital city with a line segment.\n",
    "    Can you see any regularities?\n",
    "    \n",
    "4. **[1p]** Plot 400 roughly most frequent words' embeddings (either `FastText` or `Word2vec`) in 2-D with `PCA`.\n",
    "    Skip stop words, punctuations, artifact words, etc. You can be imprecise and use heuristics\n",
    "    (e.g., select words than are at lest 3 charactes long).\n",
    "    Can you see any regularities? Another method of making meaningful visualizations is `t-SNE`.\n",
    "    \n",
    "    Make another 2-D visualization, this time using `t-SNE`. Visualizations with `t-SNE` are obtained\n",
    "    with gradient descent. Try to tweak optimization parameters to get lower optimization error,\n",
    "    than the one with default parameters.\n",
    "    Can you see any regularities this time?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
