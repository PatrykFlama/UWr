{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065b4efd",
   "metadata": {
    "id": "065b4efd"
   },
   "source": [
    "# Zadanie 1. (4+Xp)\n",
    "\n",
    "Zajmiemy się osadzeniami słów (zarówno kontekstowymi, jak i bezkontekstowymi). Uwaga: teksty, które będziemy osadzać zawsze składają się z jednego słowa (ale niekoniecznie z jednego tokenu).\n",
    "\n",
    "**a)** Zaproponuj jakiś sposób wykorzystania bezkontekstowych osadzeń tokenów (wyznaczanych przez transformer) do wyznaczenia osadzeń słów. Możesz skorzystać z programu z wykładu 7 (`embedding.ipynb`). Sprawdź, jaką jakość (mierzoną testem ABX) mają te osadzenia. Do zaliczenia zadania wymagane jest 0.6.\n",
    "\n",
    "**b)** Wykorzystaj kontekstowe osadzenia tokenów z BERT-a do wyznaczenia osadzeń dla słów. Ponownie wykonaj testy ABX.\n",
    "\n",
    "**c)** Spróbuj połączyć te dwa podejścia w jakikolwiek sposób. Jakość twojego rozwiązania przekłada się na punkty bonusowe zgodnie z wzorem: `(score − 0.6) × 6`\n",
    "\n",
    "**Procedura ewaluacji** (być może zostanie uproszczona): osadzenia zapisz w pliku tekstowym `word_embedings_file.txt`, w którym każdy wiersz wygląda tak:\n",
    "\n",
    "```\n",
    "[słowo] float_1 float_2 ... float_D\n",
    "```\n",
    "\n",
    "Osadzenia są oceniane za pomocą skryptu `word_emb_evaluation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "edce7538",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1767361692967,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "edce7538"
   },
   "outputs": [],
   "source": [
    "clusters_txt = '''\n",
    "piśmiennicze: pisak flamaster ołówek długopis pióro\n",
    "małe_ssaki: mysz szczur chomik łasica kuna bóbr\n",
    "okręty: niszczyciel lotniskowiec trałowiec krążownik pancernik fregata korweta\n",
    "lekarze: lekarz pediatra ginekolog kardiolog internista geriatra\n",
    "zupy: rosół żurek barszcz\n",
    "uczucia: miłość przyjaźń nienawiść gniew smutek radość strach\n",
    "działy_matematyki: algebra analiza topologia logika geometria\n",
    "budynki_sakralne: kościół bazylika kaplica katedra świątynia synagoga zbór\n",
    "stopień_wojskowy: chorąży podporucznik porucznik kapitan major pułkownik generał podpułkownik\n",
    "grzyby_jadalne: pieczarka borowik gąska kurka boczniak kania\n",
    "prądy_filozoficzne: empiryzm stoicyzm racjonalizm egzystencjalizm marksizm romantyzm\n",
    "religie: chrześcijaństwo buddyzm islam prawosławie protestantyzm kalwinizm luteranizm judaizm\n",
    "dzieła_muzyczne: sonata synfonia koncert preludium fuga suita\n",
    "cyfry: jedynka dwójka trójka czwórka piątka szóstka siódemka ósemka dziewiątka\n",
    "owady: ważka biedronka żuk mrówka mucha osa pszczoła chrząszcz\n",
    "broń_biała: miecz topór sztylet nóż siekiera\n",
    "broń_palna: karabin pistolet rewolwer fuzja strzelba\n",
    "komputery: komputer laptop kalkulator notebook\n",
    "kolory: biel żółć czerwień błękit zieleń brąz czerń\n",
    "duchowny: wikary biskup ksiądz proboszcz rabin pop arcybiskup kardynał pastor\n",
    "ryby: karp śledź łosoś dorsz okoń sandacz szczupak płotka\n",
    "napoje_mleczne: jogurt kefir maślanka\n",
    "czynności_sportowe: bieganie skakanie pływanie maszerowanie marsz trucht\n",
    "ubranie:  garnitur smoking frak żakiet marynarka koszula bluzka sweter sweterek sukienka kamizelka spódnica spodnie\n",
    "mebel: krzesło fotel kanapa łóżko wersalka sofa stół stolik ława\n",
    "przestępca: morderca zabójca gwałciciel złodziej bandyta kieszonkowiec łajdak łobuz\n",
    "mięso_wędliny wieprzowina wołowina baranina cielęcina boczek baleron kiełbasa szynka schab karkówka dziczyzna\n",
    "drzewo: dąb klon wiąz jesion świerk sosna modrzew platan buk cis jawor jarzębina akacja\n",
    "źródło_światła: lampa latarka lampka żyrandol żarówka reflektor latarnia lampka\n",
    "organ: wątroba płuco serce trzustka żołądek nerka macica jajowód nasieniowód prostata śledziona\n",
    "oddziały: kompania pluton batalion brygada armia dywizja pułk\n",
    "napój_alkoholowy: piwo wino wódka dżin nalewka bimber wiśniówka cydr koniak wiśniówka\n",
    "kot_drapieżny: puma pantera lampart tygrys lew ryś żbik gepard jaguar\n",
    "metal: żelazo złoto srebro miedź nikiel cyna cynk potas platyna chrom glin aluminium\n",
    "samolot: samolot odrzutowiec awionetka bombowiec myśliwiec samolocik helikopter śmigłowiec\n",
    "owoc: jabłko gruszka śliwka brzoskwinia cytryna pomarańcza grejpfrut porzeczka nektaryna\n",
    "pościel: poduszka prześcieradło kołdra kołderka poduszeczka pierzyna koc kocyk pled\n",
    "agd: lodówka kuchenka pralka zmywarka mikser sokowirówka piec piecyk piekarnik\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9e948",
   "metadata": {
    "id": "70e9e948"
   },
   "source": [
    "task a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SgOrbbGb9-No",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4344,
     "status": "ok",
     "timestamp": 1767361740924,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "SgOrbbGb9-No",
    "outputId": "2355f5e5-41cc-4783-e457-b9d0e82bf63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Embeddings shape: (50257, 768)\n",
      "Unique words: 288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "gpt2_model_name = 'gpt2'\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name).to(device)\n",
    "\n",
    "# non-contextual token embeddings from GPT-2\n",
    "gpt2_embeddings = gpt2_model.transformer.wte.weight.detach().cpu().numpy()\n",
    "print(f\"Embeddings shape: {gpt2_embeddings.shape}\")\n",
    "\n",
    "# unique words\n",
    "words = set()\n",
    "for line in test_words.split('\\n'):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    words.update(parts[1:])\n",
    "\n",
    "print(f\"Unique words: {len(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb469d46",
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1767362275046,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "eb469d46"
   },
   "outputs": [],
   "source": [
    "def get_non_contextual_word_embedding(word, tokenizer, embeddings, method='mean'):\n",
    "    \"\"\"\n",
    "    Get non-contextual word embedding by aggregating token embeddings\n",
    "    or by isolated transformer hidden states (method='combine').\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- TOKEN-LEVEL METHODS ----------\n",
    "    tokens = tokenizer.tokenize(' ' + word)\n",
    "    if not tokens:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    if not token_ids:\n",
    "        return None\n",
    "\n",
    "    token_embeddings = embeddings[token_ids]  # [T, D]\n",
    "\n",
    "    if method == 'mean':\n",
    "        return token_embeddings.mean(axis=0)\n",
    "\n",
    "    elif method == 'first':\n",
    "        return token_embeddings[0]\n",
    "\n",
    "    elif method == 'weighted':\n",
    "        size = len(token_embeddings)\n",
    "        N = min(4, size)\n",
    "\n",
    "        weights = np.array([0.1, 0.5, 0.8, 1.][-N:])\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        vec = np.zeros(token_embeddings.shape[1])\n",
    "        for i in range(N):\n",
    "            vec += token_embeddings[size - i - 1] * weights[-i-1]\n",
    "\n",
    "        return vec\n",
    "\n",
    "    elif method == 'positional':\n",
    "        D = token_embeddings.shape[1]\n",
    "        pos = np.arange(len(token_embeddings))\n",
    "        pos_enc = np.sin(pos[:, None] / (10000 ** (2 * np.arange(D) / D)))\n",
    "        return (token_embeddings * (1 + 0.1 * pos_enc)).mean(axis=0)\n",
    "\n",
    "    # ---------- TRANSFORMER-LEVEL METHODS ----------\n",
    "    elif method == 'combine':\n",
    "        gpt2_model.eval()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            ' ' + word,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=False\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model.transformer(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "        # hidden_states: tuple[L] of [1, T, D]\n",
    "        hs = torch.stack(outputs.hidden_states).squeeze(1)  # [L, T, D]\n",
    "\n",
    "        # mean of last 4 layers\n",
    "        last4 = hs[-4:].mean(dim=0)                          # [T, D]\n",
    "\n",
    "        vec = last4[-1].cpu().numpy()\n",
    "        return vec / (np.linalg.norm(vec) + 1e-9)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "e65c8cef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "f5b94add35fb4c78a74e3070551b4386",
      "a3810f1e846b4993a3d5279d581da879",
      "a33fb0de5a5242c48eaa63808539b397",
      "b3b7c41c9f9b44e091272ee54750c152",
      "70db635a2ea94fbf99e8d34c99ce4109",
      "00559fa72797476aa1cc4ae81119456e",
      "7a8d6d623e56447ea751c34c9fca6616",
      "7250be7228a4473f8c9d73af89312ab5",
      "a0ec5fc4082643e690fa55d32ce4a611",
      "92a67f5404964244b5a73f97672e178f",
      "12baa0ef9ed740399669a409350baa2f"
     ]
    },
    "executionInfo": {
     "elapsed": 36840,
     "status": "ok",
     "timestamp": 1767365094302,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "e65c8cef",
    "outputId": "f9781541-3149-4d61-a7d6-2fe72ac2f2ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b94add35fb4c78a74e3070551b4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 288/288 words\n"
     ]
    }
   ],
   "source": [
    "embeddings_1a = {}\n",
    "for word in tqdm(words):\n",
    "    emb = get_non_contextual_word_embedding(word, gpt2_tokenizer, gpt2_embeddings, method='combine')\n",
    "    if emb is not None:\n",
    "        embeddings_1a[word] = emb\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings_1a)}/{len(words)} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4e28b533",
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1767365094355,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "4e28b533"
   },
   "outputs": [],
   "source": [
    "X = np.stack(list(embeddings_1a.values()))\n",
    "mu = X.mean(axis=0)\n",
    "\n",
    "# subtract + normalize\n",
    "for w in embeddings_1a:\n",
    "    v = embeddings_1a[w] - mu\n",
    "    embeddings_1a[w] = v / (np.linalg.norm(v) + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "66260fc6",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767365094356,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "66260fc6"
   },
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings_dict, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for word, emb in embeddings_dict.items():\n",
    "            # format: word float1 float2 ... floatD\n",
    "            emb_str = ' '.join(map(str, emb))\n",
    "            f.write(f\"{word} {emb_str}\\n\")\n",
    "    print(f\"Saved {len(embeddings_dict)} embeddings to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4d5fe161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1767365094482,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "4d5fe161",
    "outputId": "3f86985e-44fd-480d-c67c-3fe5062508b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 288 embeddings to word_embedings_file.txt\n",
      "Embeddings saved\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(embeddings_1a, 'word_embedings_file.txt')\n",
    "print(\"Embeddings saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "DrsKiiEFt-yw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3523,
     "status": "ok",
     "timestamp": 1767365098006,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "DrsKiiEFt-yw",
    "outputId": "8b170811-0bd0-499c-9d45-b2fc543de2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.61432\n"
     ]
    }
   ],
   "source": [
    "!python3 word_emb_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781769bc",
   "metadata": {
    "id": "781769bc"
   },
   "source": [
    "task b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7693f9c7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1767365100580,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "7693f9c7"
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'distilbert-base-multilingual-cased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name, output_hidden_states=True).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_contextual_word_embedding(word, tokenizer, model, device, layer=-1):\n",
    "    \"\"\"\n",
    "    Get contextual word embedding from BERT.\n",
    "    Places word in minimal context to get contextual representation.\n",
    "    \"\"\"\n",
    "    text = f\"To jest {word}.\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # get embeddings from specified layer\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    layer_embedding = hidden_states[layer]\n",
    "\n",
    "    # find the tokens that correspond to the word\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    word_token_indices = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in ['[CLS]', '[PAD]', '[SEP]']:\n",
    "            continue\n",
    "\n",
    "        # check if this could be the start of our word\n",
    "        match = True\n",
    "        for j, wt in enumerate(word_tokens):\n",
    "            if i + j >= len(tokens) or tokens[i + j] != wt:\n",
    "                match = False\n",
    "                break\n",
    "        if match:\n",
    "            word_token_indices = list(range(i, i + len(word_tokens)))\n",
    "            break\n",
    "\n",
    "    # if we couldn't find the word, just take the middle tokens (skip [CLS])\n",
    "    if not word_token_indices:\n",
    "        word_token_indices = list(range(1, len(tokens) - 1))\n",
    "\n",
    "    # average embeddings of word tokens\n",
    "    if word_token_indices:\n",
    "        word_embedding = layer_embedding[0, word_token_indices, :].mean(dim=0).cpu().numpy()\n",
    "        return word_embedding\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "9ca98b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "f706829fc019497c830a836f876b798c",
      "0dc5456d00524f64af7d5d289787f9f5",
      "095d9af8a7df46e189cf991f96002bd8",
      "bf980457a1e648feb9f806a3788d4280",
      "39307f6b31594b8d938f539ae5fba3f2",
      "1f36098b6b354be4be82e948bae70906",
      "0001ff19dd44411a9323e31d1e39f8f1",
      "6c7f3b5010e94f5a962503a41071f430",
      "ab768c7671d74c02a266f9bbffd2d6f3",
      "e4a7778a2b114c22a190d9f5b1f87925",
      "bfaab097216f4b7bac4bab49cbf32c73"
     ]
    },
    "executionInfo": {
     "elapsed": 15646,
     "status": "ok",
     "timestamp": 1767365116245,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "9ca98b28",
    "outputId": "7403a1a2-b92b-48b9-90d5-925588c731d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f706829fc019497c830a836f876b798c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating contextual embeddings:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated embeddings for 288/288 words\n",
      "Saved 288 embeddings to word_embedings_file.txt\n",
      "Embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "embeddings_1b = {}\n",
    "for word in tqdm(words, desc=\"Generating contextual embeddings\"):\n",
    "    emb = get_contextual_word_embedding(word, bert_tokenizer, bert_model, device)\n",
    "    if emb is not None:\n",
    "        # normalize embedding\n",
    "        emb = emb / np.linalg.norm(emb)\n",
    "        embeddings_1b[word] = emb\n",
    "\n",
    "print(f\"Successfully generated embeddings for {len(embeddings_1b)}/{len(words)} words\")\n",
    "\n",
    "save_embeddings(embeddings_1b, 'word_embedings_file.txt')\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "89ojESWnuOKj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4835,
     "status": "ok",
     "timestamp": 1767365121081,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "89ojESWnuOKj",
    "outputId": "407be987-7e29-489b-a84c-7d182dcb30f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.688636\n"
     ]
    }
   ],
   "source": [
    "!python3 word_emb_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27a4f0",
   "metadata": {
    "id": "3e27a4f0"
   },
   "source": [
    "task c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "8ab87088",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767365121083,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "8ab87088"
   },
   "outputs": [],
   "source": [
    "def combine_embeddings(emb_noncontextual, emb_contextual, method='weighted', weight_contextual=0.5):\n",
    "    \"\"\"\n",
    "    Combine non-contextual and contextual embeddings\n",
    "    Methods: 'weighted', 'concat'\n",
    "    \"\"\"\n",
    "    if emb_noncontextual is None or emb_contextual is None:\n",
    "        return None\n",
    "\n",
    "    # normalize\n",
    "    emb_nc = emb_noncontextual / np.linalg.norm(emb_noncontextual)\n",
    "    emb_c = emb_contextual / np.linalg.norm(emb_contextual)\n",
    "\n",
    "    if method == 'weighted':\n",
    "        # weighted average\n",
    "        combined = (1 - weight_contextual) * emb_nc + weight_contextual * emb_c\n",
    "        return combined / np.linalg.norm(combined)\n",
    "    elif method == 'concat':\n",
    "        # concatenation\n",
    "        combined = np.concatenate([emb_nc, emb_c])\n",
    "        return combined / np.linalg.norm(combined)\n",
    "    else:\n",
    "        return emb_nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3e079828",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "72766ff9a7834618b691a71274dc829c",
      "e999cc245b2a4d8cb472f979a520079b",
      "e78fb6e79f5a4d229f0dc5ef11dd9f71",
      "7592a01796884cfea4f0bca0fe74ddc8",
      "825d06bd36de4b6ea2d9b55992e168f2",
      "bebd9dafb5704c02ba4efc47a85c57b2",
      "e612e1f136384991a8b867a67f3cd170",
      "1586f4f61130455d8bad84ffa3080bb6",
      "08a5f4a9285442c3b113268d54c6660f",
      "360f3f750e804ced9a533673cc7091ca",
      "4aa13ee59fe2443ea6f8f4cf42b0ea66"
     ]
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1767365121286,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "3e079828",
    "outputId": "960ee4cb-35df-41ad-eb98-18a503cbe89b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72766ff9a7834618b691a71274dc829c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating combined embeddings:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated combined embeddings for 288/288 words\n",
      "Saved 288 embeddings to word_embedings_file.txt\n",
      "Embeddings saved\n"
     ]
    }
   ],
   "source": [
    "embeddings_1c = {}\n",
    "for word in tqdm(words, desc=\"Generating combined embeddings\"):\n",
    "    if word in embeddings_1a and word in embeddings_1b:\n",
    "        combined_emb = combine_embeddings(embeddings_1a[word], embeddings_1b[word], method='weighted', weight_contextual=0.5)\n",
    "        if combined_emb is not None:\n",
    "            embeddings_1c[word] = combined_emb\n",
    "\n",
    "print(f\"Successfully generated combined embeddings for {len(embeddings_1c)}/{len(words)} words\")\n",
    "\n",
    "\n",
    "save_embeddings(embeddings_1c, 'word_embedings_file.txt')\n",
    "print(\"Embeddings saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "HSFp2cLTuO5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1767365124918,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "HSFp2cLTuO5e",
    "outputId": "772cebbd-c8b7-4d96-b164-4686e3a20d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.662298\n"
     ]
    }
   ],
   "source": [
    "!python3 word_emb_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a381e6a",
   "metadata": {
    "id": "8a381e6a"
   },
   "source": [
    "# Zadanie 2. (6+Xp)\n",
    "\n",
    "W zadaniu tym będziemy zajmować się klasyfikacją recenzji z wykorzystaniem modeli transformer, możesz tu skorzystać z programu z wykładu (`herbert.ipynb`). W tym zadaniu powinieneś użyć trzech modeli:\n",
    "\n",
    "1. Modelu generatywnego, takiego jak Papuga, Polka o wielkości do 1B, który znajduje prawdopodobieństwa tekstu (podobnie, jak na liście 1)\n",
    "2. Kodera typu BERT (np. herbert), jako ekstraktora cech\n",
    "3. Tradycyjnego modelu Machine Learning, który integruje wyniki dwóch poprzednich modeli.\n",
    "\n",
    "Ten model powinieneś wytrenować na zbiorze treningowym recenzji, a testować na testowym.\n",
    "\n",
    "Wartość premii jest równa: `20 × (a − 0.85)`, gdzie `a` to wartość accuracy na zbiorze testowym.\n",
    "\n",
    "Jeżeli chcesz, możesz skorzystać tu również z wyników kolejnego zadania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6749d2e5",
   "metadata": {
    "id": "6749d2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reviews: 400\n",
      "Training: 300\n",
      "Test: 100\n",
      "\n",
      "Sample review:\n",
      "Label: GOOD\n",
      "Text: Jedzenie doskonale.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews_file = 'reviews_for_task3.txt'\n",
    "lines = open(reviews_file).readlines()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "# preprocess data\n",
    "def parse_line(line):\n",
    "    parts = line.strip().split(maxsplit=1)\n",
    "    label = 1 if parts[0] == 'GOOD' else 0\n",
    "    text = parts[1] if len(parts) > 1 else \"\"\n",
    "    return label, text\n",
    "\n",
    "# stratified split to keep label balance\n",
    "labels = [parse_line(l)[0] for l in lines]\n",
    "train_lines, test_lines = train_test_split(\n",
    "    lines,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "N = len(lines)\n",
    "print(f\"All reviews: {N}\")\n",
    "print(f\"Training: {len(train_lines)}\")\n",
    "print(f\"Test: {len(test_lines)}\")\n",
    "\n",
    "sample_label, sample_text = parse_line(train_lines[0])\n",
    "print(f\"\\nSample review:\")\n",
    "print(f\"Label: {'GOOD' if sample_label == 1 else 'BAD'}\")\n",
    "print(f\"Text: {sample_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80044a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Loaded model\n",
      "Sample generative features: [-6.809865   -6.288423   -0.52144194  0.3725151 ]\n",
      "Loaded model\n",
      "Sample generative features: [-6.809865   -6.288423   -0.52144194  0.3725151 ]\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Papuga\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "gen_model_name = 'flax-community/papuGaPT2'\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
    "gen_model.eval()\n",
    "\n",
    "print(f\"Loaded model\")\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def avg_token_log_prob(sentence_txt, tokenizer, model):\n",
    "    input_ids = tokenizer(sentence_txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids)\n",
    "        # shift for next-token prediction\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], input_ids[:, 1:])\n",
    "        seq_log_probs = torch.sum(log_probs)\n",
    "        seq_len = input_ids.size(1) - 1\n",
    "    # average per-token log-probability (length-normalized)\n",
    "    return (seq_log_probs / max(seq_len, 1)).item()\n",
    "\n",
    "def papuga_get_opinion_features(sentence_txt, tokenizer, model):\n",
    "    good_text = \" To jest opinia pozytywna: \"\n",
    "    bad_text = \" To jest opinia negatywna: \"\n",
    "\n",
    "    sent_good = sentence_txt + good_text\n",
    "    sent_bad = sentence_txt + bad_text\n",
    "\n",
    "    avg_logp_good = avg_token_log_prob(sent_good, tokenizer, model)\n",
    "    avg_logp_bad = avg_token_log_prob(sent_bad, tokenizer, model)\n",
    "\n",
    "    # difference and softmax-like probability between the two prompts\n",
    "    diff = avg_logp_good - avg_logp_bad\n",
    "    p_good = np.exp(avg_logp_good) / (np.exp(avg_logp_good) + np.exp(avg_logp_bad))\n",
    "\n",
    "    # Return a compact, informative feature vector\n",
    "    return np.array([avg_logp_good, avg_logp_bad, diff, p_good], dtype=np.float32)\n",
    "\n",
    "sample_features = papuga_get_opinion_features(sample_text, gen_tokenizer, gen_model)\n",
    "print(f\"Sample generative features: {sample_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e004d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "084b2b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "BERT feature shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Model 2: BERT (Herbert)\n",
    "from transformers import AutoModel\n",
    "\n",
    "herbert_model_name = 'allegro/herbert-base-cased'\n",
    "herbert_tokenizer = AutoTokenizer.from_pretrained(herbert_model_name)\n",
    "herbert_model = AutoModel.from_pretrained(herbert_model_name).to(device)\n",
    "herbert_model.eval()\n",
    "\n",
    "print(f\"Loaded model\")\n",
    "\n",
    "def get_bert_features(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state[0]  # [seq_len, hidden]\n",
    "        mask = inputs['attention_mask'][0].unsqueeze(-1).float()  # [seq_len, 1]\n",
    "        # mean-pooling over valid tokens (often stronger than CLS)\n",
    "        pooled = (last_hidden * mask).sum(dim=0) / mask.sum()\n",
    "\n",
    "    return pooled.cpu().numpy()\n",
    "\n",
    "\n",
    "sample_bert_features = get_bert_features(sample_text, herbert_tokenizer, herbert_model)\n",
    "print(f\"BERT feature shape: {sample_bert_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca224dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8425e5be41348f48e41957a3c57ba61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training features:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set feature shapes:\n",
      "  Generative features: (1200, 4)\n",
      "  BERT features: (1200, 768)\n",
      "  Combined features: (1200, 772)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def spoil(L):\n",
    "    res = []\n",
    "    for w in L:\n",
    "        if random.random() < 0.85:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            res.append(w.upper())\n",
    "    return res\n",
    "\n",
    "X_train_gen = []\n",
    "X_train_bert = []\n",
    "y_train = []\n",
    "\n",
    "for i, line in enumerate(tqdm(train_lines, desc=\"Training features\")):\n",
    "    label, text = parse_line(line)\n",
    "\n",
    "    # original\n",
    "    gen_features = papuga_get_opinion_features(text, gen_tokenizer, gen_model)\n",
    "    X_train_gen.append(gen_features)\n",
    "\n",
    "    bert_features = get_bert_features(text, herbert_tokenizer, herbert_model)\n",
    "    X_train_bert.append(bert_features)\n",
    "    y_train.append(label)\n",
    "\n",
    "    # add spoils\n",
    "    for _ in range(3):\n",
    "        text_spoiled = ' '.join(spoil(text.split()))\n",
    "        gen_features_sp = papuga_get_opinion_features(text_spoiled, gen_tokenizer, gen_model)\n",
    "        X_train_gen.append(gen_features_sp)\n",
    "\n",
    "        bert_features_sp = get_bert_features(text_spoiled, herbert_tokenizer, herbert_model)\n",
    "        X_train_bert.append(bert_features_sp)\n",
    "        y_train.append(label)\n",
    "\n",
    "X_train_gen = np.array(X_train_gen)\n",
    "X_train_bert = np.array(X_train_bert)\n",
    "X_train_combined = np.concatenate([X_train_gen, X_train_bert], axis=1)\n",
    "\n",
    "print(f\"\\nTraining set feature shapes:\")\n",
    "print(f\"  Generative features: {X_train_gen.shape}\")\n",
    "print(f\"  BERT features: {X_train_bert.shape}\")\n",
    "print(f\"  Combined features: {X_train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcef42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce1b4f26d454fb0b03c516943e1bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test features:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set feature shapes:\n",
      "  Generative features: (100, 4)\n",
      "  BERT features: (100, 768)\n",
      "  Combined features: (100, 772)\n"
     ]
    }
   ],
   "source": [
    "X_test_gen = []\n",
    "X_test_bert = []\n",
    "y_test = []\n",
    "\n",
    "for line in tqdm(test_lines, desc=\"Test features\"):\n",
    "    label, text = parse_line(line)\n",
    "\n",
    "    # get generative model features\n",
    "    gen_features = papuga_get_opinion_features(text, gen_tokenizer, gen_model)\n",
    "    X_test_gen.append(gen_features)\n",
    "    \n",
    "    # get BERT features\n",
    "    bert_features = get_bert_features(text, herbert_tokenizer, herbert_model)\n",
    "    X_test_bert.append(bert_features)\n",
    "    \n",
    "    y_test.append(label)\n",
    "\n",
    "# combine features\n",
    "X_test_gen = np.array(X_test_gen)\n",
    "X_test_bert = np.array(X_test_bert)\n",
    "X_test_combined = np.concatenate([X_test_gen, X_test_bert], axis=1)\n",
    "\n",
    "print(f\"\\nTest set feature shapes:\")\n",
    "print(f\"  Generative features: {X_test_gen.shape}\")\n",
    "print(f\"  BERT features: {X_test_bert.shape}\")\n",
    "print(f\"  Combined features: {X_test_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb681206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS - Combined Model (Generative + BERT + LogReg)\n",
      "==================================================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.8200\n",
      "\n",
      "Bonus points: -0.60p\n",
      "==================================================\n",
      "\n",
      "Detailed Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.81      0.84      0.82        50\n",
      "        GOOD       0.83      0.80      0.82        50\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.82      0.82      0.82       100\n",
      "weighted avg       0.82      0.82      0.82       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Train Logistic Regression on combined features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "X_test_scaled = scaler.transform(X_test_combined)\n",
    "\n",
    "# logistic regression\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, C=0.5, class_weight='balanced')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predictions\n",
    "y_train_pred = clf.predict(X_train_scaled)\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS - Combined Model (Generative + BERT + LogReg)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"\\nBonus points: {20 * (test_accuracy - 0.85):.2f}p\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\nDetailed Test Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['BAD', 'GOOD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d2007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of different feature combinations:\n",
      "\n",
      "BERT only:              0.8200\n",
      "Generative only:        0.6600\n",
      "Combined (Gen + BERT):  0.8200\n",
      "\n",
      "Improvement from combining: 0.0000\n",
      "BERT only:              0.8200\n",
      "Generative only:        0.6600\n",
      "Combined (Gen + BERT):  0.8200\n",
      "\n",
      "Improvement from combining: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# individual models vs combined\n",
    "print(\"Comparison of different feature combinations:\\n\")\n",
    "\n",
    "# train on BERT features only\n",
    "clf_bert = LogisticRegression(random_state=42, max_iter=1000)\n",
    "X_train_bert_scaled = scaler.fit_transform(X_train_bert)\n",
    "X_test_bert_scaled = scaler.transform(X_test_bert)\n",
    "clf_bert.fit(X_train_bert_scaled, y_train)\n",
    "bert_test_acc = accuracy_score(y_test, clf_bert.predict(X_test_bert_scaled))\n",
    "print(f\"BERT only:              {bert_test_acc:.4f}\")\n",
    "\n",
    "# train on Generative features only\n",
    "clf_gen = LogisticRegression(random_state=42, max_iter=1000)\n",
    "X_train_gen_scaled = scaler.fit_transform(X_train_gen)\n",
    "X_test_gen_scaled = scaler.transform(X_test_gen)\n",
    "clf_gen.fit(X_train_gen_scaled, y_train)\n",
    "gen_test_acc = accuracy_score(y_test, clf_gen.predict(X_test_gen_scaled))\n",
    "print(f\"Generative only:        {gen_test_acc:.4f}\")\n",
    "\n",
    "print(f\"Combined (Gen + BERT):  {test_accuracy:.4f}\")\n",
    "print(f\"\\nImprovement from combining: {test_accuracy - max(bert_test_acc, gen_test_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33e7d2",
   "metadata": {
    "id": "9d33e7d2"
   },
   "source": [
    "# Zadanie 3. (8+1p)\n",
    "\n",
    "W tym zadaniu powinieneś sprawdzić, czy augmentacja danych może poprawić wyniki klasyfikacji, w której BERT jest traktowany jako ekstraktor cech. Mamy 3 osobno punktowane procedury generowania nowych wariantów recenzji:\n",
    "\n",
    "**a)** Augmentacja mechaniczna (czyli wprowadzasz jakieś zniekształcenia w tekście, mogą to być na przykład literówki, zmiana wielkości liter, błędy związane z polskimi literami, etc). **(2p)**\n",
    "\n",
    "**b)** Augmentacja modelem generatywnym, na przykład Papugą. Powinieneś generować recenzje, które bazują na oryginalnej recenzji, zachowując jej polarność (czyli to, czy jest pozytywna, czy negatywna). Zwróć uwagę, że „fantazja\" modelu językowego nie musi tu być wadą – tak naprawdę to niekoniecznie w tej procedurze muszą powstawać poprawne teksty. **(3p)**\n",
    "\n",
    "**c)** Ta procedura augmentacji powinna bazować na Word2Vec i zachowywać w miarę możliwości znaczenie tekstu. Należy wybrane słowa zamieniać na słowa bliskoznaczne, w tej samej formie gramatycznej (będzie to dokładniej omówione na kolejnym wykładzie). **(3p)**\n",
    "\n",
    "Przykładowo recenzja:\n",
    "- *Hotel ogólnie bardzo ładny.* mogłaby być zmieniona na *Pensjonat szczególnie bardzo piękny.*\n",
    "- *Polecam wszystkim tego fizjoterapeutę!* na *Rekomenduję wszystkim tego ortopedę!*\n",
    "\n",
    "Konieczne informacje gramatyczne pojawią się na wykładzie 8 (czyli najbliższym).\n",
    "\n",
    "---\n",
    "\n",
    "Każda recenzja powinna posłużyć do wygenerowania K innych recenzji (dobór K to Twoje zadanie), stąd należy generator napisać w ten sposób, by recenzje były tworzone niedeterministycznie.\n",
    "\n",
    "Dla wybranych (lub wszystkich) procedur przeprowadź uczenie na zaugmentowanych danych za pomocą regresji logistycznej. Dodatkowo można uzyskać **1p premii**, jeżeli któraś z procedur da korzyść w porównaniu do oryginalnych danych (tzn. dzięki augmentacji uda się uzyskać lepszy wynik dla danych testowych).\n",
    "\n",
    "W zadaniu do maksimum wlicza się **6p**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed3b4e",
   "metadata": {},
   "source": [
    "## Task 3a - Mechanical Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hotel ogólnie bardzo ładny.\n",
      "\n",
      "Mechanical augmentations:\n",
      "1. Htoel ogólnie bardzo ładny.\n",
      "2. hotel ogólnie bardzo ładyn.\n",
      "3. HotEl ogólnie bardzo łAdny.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def mechanical_augmentation(text, num_variants=3):\n",
    "    variants = []\n",
    "    \n",
    "    # de-polish'ify\n",
    "    polish_chars = {\n",
    "        'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l', \n",
    "        'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z',\n",
    "        'Ą': 'A', 'Ć': 'C', 'Ę': 'E', 'Ł': 'L',\n",
    "        'Ń': 'N', 'Ó': 'O', 'Ś': 'S', 'Ź': 'Z', 'Ż': 'Z'\n",
    "    }\n",
    "    \n",
    "    for _ in range(num_variants):\n",
    "        chars = list(text)\n",
    "\n",
    "        num_changes = random.randint(1, max(2, len(text) // 10))\n",
    "\n",
    "        for _ in range(num_changes):\n",
    "            transform_type = random.choices(\n",
    "                ['case', 'polish', 'typo', 'space'],\n",
    "                weights=[0.3, 0.3, 0.1, 0.2],\n",
    "                k=1\n",
    "            )[0]\n",
    "\n",
    "            if len(chars) < 3:\n",
    "                break\n",
    "\n",
    "            pos = random.randint(0, len(chars) - 1)\n",
    "\n",
    "            # case change\n",
    "            if transform_type == 'case' and chars[pos].isalpha():\n",
    "                chars[pos] = chars[pos].swapcase()\n",
    "\n",
    "            # de-polish'ify\n",
    "            elif transform_type == 'polish' and chars[pos] in polish_chars:\n",
    "                chars[pos] = polish_chars[chars[pos]]\n",
    "        \n",
    "            # typo\n",
    "            elif transform_type == 'typo' and chars[pos].isalpha():\n",
    "                typo_type = random.choice(['swap', 'subs', 'delete', 'duplicate'])\n",
    "        \n",
    "                # swap adjacent characters\n",
    "                if typo_type == 'swap' and pos < len(chars) - 1:\n",
    "                    chars[pos], chars[pos + 1] = chars[pos + 1], chars[pos]\n",
    "            \n",
    "                # substitute character\n",
    "                elif typo_type == 'subs':\n",
    "                    substitute_char = random.randint(ord('a'), ord('z'))\n",
    "\n",
    "                    if chars[pos].isupper():\n",
    "                        substitute_char = chr(substitute_char).upper()\n",
    "                    else:\n",
    "                        substitute_char = chr(substitute_char)\n",
    "\n",
    "                    chars[pos] = substitute_char\n",
    "\n",
    "                # delete character\n",
    "                elif typo_type == 'delete':\n",
    "                    chars.pop(pos)\n",
    "            \n",
    "                # duplicate character\n",
    "                elif typo_type == 'duplicate':\n",
    "                    chars.insert(pos, chars[pos])\n",
    "            \n",
    "            # remove or add space\n",
    "            elif transform_type == 'space' and chars[pos] == ' ':\n",
    "                if random.random() > 0.5:\n",
    "                    chars.pop(pos)\n",
    "                else:\n",
    "                    chars.insert(pos, ' ')\n",
    "\n",
    "        augmented = ''.join(chars)\n",
    "\n",
    "        if augmented and augmented != text:\n",
    "            variants.append(augmented)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "sample_text = \"Hotel ogólnie bardzo ładny. Polecam wszystkim tego fizjoterapeutę!\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"\\nMechanical augmentations:\")\n",
    "for i, variant in enumerate(mechanical_augmentation(sample_text, 5), 1):\n",
    "    print(f\"{i}. {variant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd08740",
   "metadata": {},
   "source": [
    "## Task 3b - Generative Model Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb8f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review:\n",
      "Label: BAD\n",
      "Text: Najgorsze w tym hotelu było że w szczytowym okresie ferii mazowieckim są robione remonty było głośno i pełno robotników poruszających po obiekcie.\n",
      "\n",
      "Generative augmentations:\n",
      "1. Najgorsze w tym hotelu było że nie mogliśmy korzystać z ręczników i pościeli. Wszystko co brudne to ręczniki ale nic złego się stało ,zabrakło tylko kawy do kolacji no bo przecież trzeba iść na kawę...\n",
      "Byłam tam dwa razy ( raz jak był remont) a za drugim razem\n",
      "\n",
      "2. Recenzja negatywna: Najgorsze w tym hotelarstwie jest to, że wszyscy mówią jednym głosem - Wstyd i Zazdrość.\n",
      "Najbardziej obrzydliwy hotel na świecie! Czysta woda plus syf a nawet więcej niż się wydaje nie wiem... Hotel był ok ale tylko za kasę z\n",
      "\n",
      "3. Najgorsze w tym hotelu było że w szczyto nie wiem co jest windą a na śniadanie i kawę wychodziłem z pokoju o 6:30, tak więc spóźniony. Po wejściu do mieszkania przywitał mnie niemiły kelner który kazał nam czekać aż będziemy sami u niego po coś jechać . I\n",
      "\n",
      "1. Najgorsze w tym hotelu było że nie mogliśmy korzystać z ręczników i pościeli. Wszystko co brudne to ręczniki ale nic złego się stało ,zabrakło tylko kawy do kolacji no bo przecież trzeba iść na kawę...\n",
      "Byłam tam dwa razy ( raz jak był remont) a za drugim razem\n",
      "\n",
      "2. Recenzja negatywna: Najgorsze w tym hotelarstwie jest to, że wszyscy mówią jednym głosem - Wstyd i Zazdrość.\n",
      "Najbardziej obrzydliwy hotel na świecie! Czysta woda plus syf a nawet więcej niż się wydaje nie wiem... Hotel był ok ale tylko za kasę z\n",
      "\n",
      "3. Najgorsze w tym hotelu było że w szczyto nie wiem co jest windą a na śniadanie i kawę wychodziłem z pokoju o 6:30, tak więc spóźniony. Po wejściu do mieszkania przywitał mnie niemiły kelner który kazał nam czekać aż będziemy sami u niego po coś jechać . I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generative_augmentation(text, label, tokenizer, model, device, num_variants=3, max_length=100):\n",
    "    variants = []\n",
    "\n",
    "    # prompts based on label\n",
    "    sentiment = \"pozytywna\" if label == 1 else \"negatywna\"\n",
    "\n",
    "    # random prompt templates\n",
    "    templates = [\n",
    "        f\"{text[:30]}\",\n",
    "        f\"Recenzja {sentiment}: {text[:20]}\",\n",
    "    ]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for template in templates[:num_variants]:\n",
    "        inputs = tokenizer(template, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_length=min(max_length, len(inputs['input_ids'][0]) + 50),\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=random.uniform(0.8, 1.2),\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        generated = generated.strip()\n",
    "        generated = re.split(r'(?<=[.!?]) +', generated)[0]     # take first sentence\n",
    "        if generated and generated != text and len(generated) > 10:\n",
    "            variants.append(generated)\n",
    "\n",
    "    return variants\n",
    "\n",
    "\n",
    "sample_label, sample_text = parse_line(train_lines[0])\n",
    "print(\"Original review:\")\n",
    "print(f\"Label: {'GOOD' if sample_label == 1 else 'BAD'}\")\n",
    "print(f\"Text: {sample_text}\\n\")\n",
    "\n",
    "print(\"Generative augmentations:\")\n",
    "gen_variants = generative_augmentation(sample_text, sample_label, gen_tokenizer, gen_model, device, num_variants=3)\n",
    "for i, variant in enumerate(gen_variants, 1):\n",
    "    print(f\"{i}. {variant}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec657e49",
   "metadata": {},
   "source": [
    "## Task 3c - Word2Vec Augmentation with Grammatical Form Preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd829286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m19.8/27.9 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mRequirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n",
      "Successfully installed gensim-4.4.0\n",
      "Collecting pl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.8.0/pl_core_news_sm-3.8.0-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/20.2 MB\u001b[0m \u001b[31m189.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mCollecting pl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.8.0/pl_core_news_sm-3.8.0-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pl-core-news-sm\n",
      "Installing collected packages: pl-core-news-sm\n",
      "Successfully installed pl-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_sm')\n",
      "Successfully installed pl-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim spacy\n",
    "!python -m spacy download pl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc9de41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Polish Word2Vec model...\n",
      "[===================================---------------] 71.7% 1191.7/1662.8MB downloadedCould not load Word2Vec model\n",
      "[===================================---------------] 71.7% 1191.7/1662.8MB downloadedCould not load Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Polish spaCy model for morphological analysis\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "# Download Polish Word2Vec model\n",
    "print(\"Downloading Polish Word2Vec model...\")\n",
    "# Using a Polish word2vec model - this might take a while\n",
    "# Alternative: train on the review corpus itself\n",
    "try:\n",
    "    word2vec_model = api.load('word2vec-google-news-300')  # English fallback\n",
    "    print(\"Loaded English Word2Vec (fallback)\")\n",
    "except:\n",
    "    print(\"Could not load Word2Vec model\")\n",
    "    word2vec_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2c047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec on 300 reviews...\n",
      "Vocabulary size: 472\n",
      "\n",
      "Original review:\n",
      "Label: BAD\n",
      "Text: Najgorsze w tym hotelu było że w szczytowym okresie ferii mazowieckim są robione remonty było głośno i pełno robotników poruszających po obiekcie.\n",
      "\n",
      "Word2Vec augmentations:\n",
      "1. Najgorsze w tym hotelu się że w szczytowym okresie ferii mazowieckim są robione remonty było zadbać i pełno robotników poruszających po obiekcie.\n",
      "2. Najgorsze w tym w się że w szczytowym okresie ferii mazowieckim są robione remonty było głośno i pełno robotników poruszających po obiekcie.\n",
      "3. Najgorsze w tym w było że w szczytowym okresie ferii mazowieckim są robione remonty było głośno i pełno robotników poruszających po obiekcie.\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec on our review corpus for better Polish support\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare corpus from reviews\n",
    "corpus = []\n",
    "for line in train_lines:\n",
    "    _, text = parse_line(line)\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    corpus.append(tokens)\n",
    "\n",
    "print(f\"Training Word2Vec on {len(corpus)} reviews...\")\n",
    "w2v_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")\n",
    "\n",
    "def find_similar_word_with_form(word, doc_token, w2v_model, top_n=10):\n",
    "    \"\"\"\n",
    "    Find similar word and try to match grammatical form.\n",
    "    Uses lemma for similarity search, then tries to match morphological tags.\n",
    "    \"\"\"\n",
    "    lemma = doc_token.lemma_.lower()\n",
    "    \n",
    "    # Get similar words based on lemma\n",
    "    try:\n",
    "        similar_words = w2v_model.wv.most_similar(lemma, topn=top_n)\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "    # Get morphological features\n",
    "    original_pos = doc_token.pos_\n",
    "    original_morph = doc_token.morph\n",
    "    \n",
    "    # Try to find a similar word with matching POS\n",
    "    for similar_word, similarity in similar_words:\n",
    "        # Skip if too similar (same word)\n",
    "        if similar_word == lemma:\n",
    "            continue\n",
    "            \n",
    "        # Simple heuristic: use the similar word as-is\n",
    "        # In production, you'd use a morphological generator here\n",
    "        return similar_word\n",
    "    \n",
    "    return None\n",
    "\n",
    "def word2vec_augmentation(text, w2v_model, nlp, num_variants=3, replacement_prob=0.3):\n",
    "    \"\"\"\n",
    "    Generate augmented versions using Word2Vec synonym replacement.\n",
    "    Attempts to preserve grammatical form.\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for _ in range(num_variants):\n",
    "        new_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation and spaces\n",
    "            if token.is_punct or token.is_space:\n",
    "                new_tokens.append(token.text)\n",
    "                continue\n",
    "            \n",
    "            # Randomly decide whether to replace this word\n",
    "            if random.random() < replacement_prob:\n",
    "                # Only replace nouns, adjectives, verbs, and adverbs\n",
    "                if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "                    replacement = find_similar_word_with_form(token.text, token, w2v_model)\n",
    "                    if replacement:\n",
    "                        new_tokens.append(replacement)\n",
    "                    else:\n",
    "                        new_tokens.append(token.text)\n",
    "                else:\n",
    "                    new_tokens.append(token.text)\n",
    "            else:\n",
    "                new_tokens.append(token.text)\n",
    "        \n",
    "        # Reconstruct text preserving spaces\n",
    "        augmented = \"\"\n",
    "        for i, token in enumerate(doc):\n",
    "            if i < len(new_tokens):\n",
    "                augmented += new_tokens[i]\n",
    "                if i < len(doc) - 1 and not doc[i + 1].is_punct:\n",
    "                    augmented += \" \"\n",
    "        \n",
    "        augmented = augmented.strip()\n",
    "        \n",
    "        if augmented and augmented != text:\n",
    "            variants.append(augmented)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "# Test Word2Vec augmentation\n",
    "print(\"\\nOriginal review:\")\n",
    "print(f\"Label: {'GOOD' if sample_label == 1 else 'BAD'}\")\n",
    "print(f\"Text: {sample_text}\\n\")\n",
    "\n",
    "print(\"Word2Vec augmentations:\")\n",
    "w2v_variants = word2vec_augmentation(sample_text, w2v_model, nlp, num_variants=3)\n",
    "for i, variant in enumerate(w2v_variants, 1):\n",
    "    print(f\"{i}. {variant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf664c6",
   "metadata": {},
   "source": [
    "## Apply Augmentation and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06706703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data size: 300\n",
      "Original test data size: 100\n"
     ]
    }
   ],
   "source": [
    "def create_augmented_dataset(lines, augmentation_fn, aug_params, K=2):\n",
    "    \"\"\"\n",
    "    Create augmented dataset by applying augmentation function.\n",
    "    \n",
    "    Args:\n",
    "        lines: Original data lines\n",
    "        augmentation_fn: Function to generate augmented samples\n",
    "        aug_params: Dictionary of parameters to pass to augmentation_fn\n",
    "        K: Number of augmented samples per original sample\n",
    "    \n",
    "    Returns:\n",
    "        augmented_lines: List of augmented samples (label, text) tuples\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    for line in tqdm(lines, desc=\"Augmenting data\"):\n",
    "        label, text = parse_line(line)\n",
    "        \n",
    "        # Keep original\n",
    "        augmented_data.append((label, text))\n",
    "        \n",
    "        # Generate K augmented versions\n",
    "        try:\n",
    "            variants = augmentation_fn(text, **aug_params)\n",
    "            \n",
    "            # Take up to K variants\n",
    "            for variant in variants[:K]:\n",
    "                augmented_data.append((label, variant))\n",
    "        except Exception as e:\n",
    "            # If augmentation fails, just use original\n",
    "            continue\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "print(f\"Original training data size: {len(train_lines)}\")\n",
    "print(f\"Original test data size: {len(test_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6807a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Creating Augmented Datasets\n",
      "============================================================\n",
      "\n",
      "1. Mechanical Augmentation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409ddbd706d0408cacc2270e8de927fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting data:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Augmented dataset size: 723\n",
      "\n",
      "2. Generative Model Augmentation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c7c450729548eda951e1744c42ef48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Generating:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Augmented dataset size: 900\n",
      "\n",
      "3. Word2Vec Augmentation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccca1e5301e441580a33fa16d0c4383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting data:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Augmented dataset size: 713\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create augmented datasets for each method\n",
    "K = 2  # Number of augmented samples per original\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating Augmented Datasets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mechanical augmentation\n",
    "print(\"\\n1. Mechanical Augmentation...\")\n",
    "augmented_mechanical = create_augmented_dataset(\n",
    "    train_lines,\n",
    "    mechanical_augmentation,\n",
    "    {'num_variants': K},\n",
    "    K=K\n",
    ")\n",
    "print(f\"   Augmented dataset size: {len(augmented_mechanical)}\")\n",
    "\n",
    "# Generative augmentation\n",
    "print(\"\\n2. Generative Model Augmentation...\")\n",
    "def gen_aug_wrapper(text, label, num_variants=K):\n",
    "    return generative_augmentation(text, label, gen_tokenizer, gen_model, device, num_variants)\n",
    "\n",
    "augmented_generative = []\n",
    "for line in tqdm(train_lines, desc=\"   Generating\"):\n",
    "    label, text = parse_line(line)\n",
    "    augmented_generative.append((label, text))\n",
    "    \n",
    "    try:\n",
    "        variants = gen_aug_wrapper(text, label, num_variants=K)\n",
    "        for variant in variants[:K]:\n",
    "            augmented_generative.append((label, variant))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"   Augmented dataset size: {len(augmented_generative)}\")\n",
    "\n",
    "# Word2Vec augmentation\n",
    "print(\"\\n3. Word2Vec Augmentation...\")\n",
    "augmented_word2vec = create_augmented_dataset(\n",
    "    train_lines,\n",
    "    word2vec_augmentation,\n",
    "    {'w2v_model': w2v_model, 'nlp': nlp, 'num_variants': K},\n",
    "    K=K\n",
    ")\n",
    "print(f\"   Augmented dataset size: {len(augmented_word2vec)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35afe14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting BERT features for augmented datasets...\n",
      "This may take a while...\n",
      "\n",
      "1. Mechanical augmentation features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70da79fe47b4feda7ab408a281b6b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Generative augmentation features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf435fd5ec04aa291d51e09c6e28839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Word2Vec augmentation features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286f96ddb0547ff9b53577eb1f5911c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete!\n",
      "Mechanical: (723, 768)\n",
      "Generative: (900, 768)\n",
      "Word2Vec: (713, 768)\n"
     ]
    }
   ],
   "source": [
    "def extract_features_from_dataset(data, gen_tokenizer, gen_model, herbert_tokenizer, herbert_model, device):\n",
    "    \"\"\"\n",
    "    Extract BERT features from augmented dataset.\n",
    "    For efficiency, we'll use only BERT features for augmented training.\n",
    "    \"\"\"\n",
    "    X_bert = []\n",
    "    y = []\n",
    "    \n",
    "    for label, text in tqdm(data, desc=\"Extracting features\"):\n",
    "        # Get BERT features (primary features for augmentation experiments)\n",
    "        bert_features = get_bert_features(text, herbert_tokenizer, herbert_model)\n",
    "        X_bert.append(bert_features)\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X_bert), np.array(y)\n",
    "\n",
    "print(\"\\nExtracting BERT features for augmented datasets...\")\n",
    "print(\"This may take a while...\\n\")\n",
    "\n",
    "# Extract features for each augmented dataset\n",
    "print(\"1. Mechanical augmentation features...\")\n",
    "X_train_mech, y_train_mech = extract_features_from_dataset(\n",
    "    augmented_mechanical, gen_tokenizer, gen_model, \n",
    "    herbert_tokenizer, herbert_model, device\n",
    ")\n",
    "\n",
    "print(\"2. Generative augmentation features...\")\n",
    "X_train_gen_aug, y_train_gen_aug = extract_features_from_dataset(\n",
    "    augmented_generative, gen_tokenizer, gen_model,\n",
    "    herbert_tokenizer, herbert_model, device\n",
    ")\n",
    "\n",
    "print(\"3. Word2Vec augmentation features...\")\n",
    "X_train_w2v, y_train_w2v = extract_features_from_dataset(\n",
    "    augmented_word2vec, gen_tokenizer, gen_model,\n",
    "    herbert_tokenizer, herbert_model, device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction complete!\")\n",
    "print(f\"Mechanical: {X_train_mech.shape}\")\n",
    "print(f\"Generative: {X_train_gen_aug.shape}\")\n",
    "print(f\"Word2Vec: {X_train_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede7949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING AND EVALUATION WITH AUGMENTATION\n",
      "======================================================================\n",
      "\n",
      "                BASELINE (No Augmentation - BERT only)                \n",
      "----------------------------------------------------------------------\n",
      "Test Accuracy: 0.7300\n",
      "\n",
      "                       Mechanical Augmentation                        \n",
      "----------------------------------------------------------------------\n",
      "Training set size: 723\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.7500\n",
      "Improvement over baseline: +0.0200\n",
      "✓ BONUS: This method improved the baseline! (+0.0200)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.73      0.71      0.72        45\n",
      "        GOOD       0.77      0.78      0.77        55\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.75      0.75      0.75       100\n",
      "weighted avg       0.75      0.75      0.75       100\n",
      "\n",
      "\n",
      "                    Generative Model Augmentation                     \n",
      "----------------------------------------------------------------------\n",
      "Training set size: 723\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.7500\n",
      "Improvement over baseline: +0.0200\n",
      "✓ BONUS: This method improved the baseline! (+0.0200)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.73      0.71      0.72        45\n",
      "        GOOD       0.77      0.78      0.77        55\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.75      0.75      0.75       100\n",
      "weighted avg       0.75      0.75      0.75       100\n",
      "\n",
      "\n",
      "                    Generative Model Augmentation                     \n",
      "----------------------------------------------------------------------\n",
      "Training set size: 900\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.7300\n",
      "Improvement over baseline: +0.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.70      0.69      0.70        45\n",
      "        GOOD       0.75      0.76      0.76        55\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.73      0.73      0.73       100\n",
      "weighted avg       0.73      0.73      0.73       100\n",
      "\n",
      "\n",
      "                        Word2Vec Augmentation                         \n",
      "----------------------------------------------------------------------\n",
      "Training set size: 713\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.7500\n",
      "Improvement over baseline: +0.0200\n",
      "✓ BONUS: This method improved the baseline! (+0.0200)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.73      0.71      0.72        45\n",
      "        GOOD       0.77      0.78      0.77        55\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.75      0.75      0.75       100\n",
      "weighted avg       0.75      0.75      0.75       100\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SUMMARY OF RESULTS\n",
      "======================================================================\n",
      "baseline                 : 0.7300  (Δ = +0.0000)\n",
      "mechanical               : 0.7500  (Δ = +0.0200)\n",
      "generative_model         : 0.7300  (Δ = +0.0000)\n",
      "word2vec                 : 0.7500  (Δ = +0.0200)\n",
      "======================================================================\n",
      "Training set size: 900\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.7300\n",
      "Improvement over baseline: +0.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.70      0.69      0.70        45\n",
      "        GOOD       0.75      0.76      0.76        55\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.73      0.73      0.73       100\n",
      "weighted avg       0.73      0.73      0.73       100\n",
      "\n",
      "\n",
      "                        Word2Vec Augmentation                         \n",
      "----------------------------------------------------------------------\n",
      "Training set size: 713\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.7500\n",
      "Improvement over baseline: +0.0200\n",
      "✓ BONUS: This method improved the baseline! (+0.0200)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.73      0.71      0.72        45\n",
      "        GOOD       0.77      0.78      0.77        55\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.75      0.75      0.75       100\n",
      "weighted avg       0.75      0.75      0.75       100\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SUMMARY OF RESULTS\n",
      "======================================================================\n",
      "baseline                 : 0.7300  (Δ = +0.0000)\n",
      "mechanical               : 0.7500  (Δ = +0.0200)\n",
      "generative_model         : 0.7300  (Δ = +0.0000)\n",
      "word2vec                 : 0.7500  (Δ = +0.0200)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models on augmented data\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING AND EVALUATION WITH AUGMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We already have test features from Task 2\n",
    "# X_test_bert_scaled and y_test\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Baseline (from Task 2 - BERT only)\n",
    "print(f\"\\n{'BASELINE (No Augmentation - BERT only)':^70}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Test Accuracy: {bert_test_acc:.4f}\")\n",
    "results['baseline'] = bert_test_acc\n",
    "\n",
    "# Train models with each augmentation method\n",
    "augmentation_methods = [\n",
    "    ('Mechanical', X_train_mech, y_train_mech),\n",
    "    ('Generative Model', X_train_gen_aug, y_train_gen_aug),\n",
    "    ('Word2Vec', X_train_w2v, y_train_w2v)\n",
    "]\n",
    "\n",
    "for method_name, X_aug, y_aug in augmentation_methods:\n",
    "    print(f\"\\n{method_name + ' Augmentation':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler_aug = StandardScaler()\n",
    "    X_aug_scaled = scaler_aug.fit_transform(X_aug)\n",
    "    X_test_scaled_aug = scaler_aug.transform(X_test_bert)\n",
    "    \n",
    "    # Train logistic regression\n",
    "    clf_aug = LogisticRegression(random_state=42, max_iter=1000, C=1.0)\n",
    "    clf_aug.fit(X_aug_scaled, y_aug)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred_aug = clf_aug.predict(X_aug_scaled)\n",
    "    y_test_pred_aug = clf_aug.predict(X_test_scaled_aug)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc_aug = accuracy_score(y_aug, y_train_pred_aug)\n",
    "    test_acc_aug = accuracy_score(y_test, y_test_pred_aug)\n",
    "    \n",
    "    print(f\"Training set size: {len(X_aug)}\")\n",
    "    print(f\"Train Accuracy: {train_acc_aug:.4f}\")\n",
    "    print(f\"Test Accuracy:  {test_acc_aug:.4f}\")\n",
    "    \n",
    "    improvement = test_acc_aug - bert_test_acc\n",
    "    print(f\"Improvement over baseline: {improvement:+.4f}\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"✓ BONUS: This method improved the baseline! (+{improvement:.4f})\")\n",
    "    \n",
    "    results[method_name.lower().replace(' ', '_')] = test_acc_aug\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred_aug, target_names=['BAD', 'GOOD']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for method, acc in results.items():\n",
    "    improvement = acc - results['baseline'] if method != 'baseline' else 0\n",
    "    print(f\"{method:25s}: {acc:.4f}  (Δ = {improvement:+.4f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db087efd",
   "metadata": {},
   "source": [
    "## Combined Augmentation Strategy\n",
    "\n",
    "Let's try combining multiple augmentation methods to see if we can get even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd45de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined augmented dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b76ea3285a49d2b0744b3086f74489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combined augmentation:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined augmented dataset size: 1002\n",
      "\n",
      "Extracting features for combined dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27ea54d49064bbdbabbe13688538f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/1002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Combined Augmentation (All Methods)                  \n",
      "----------------------------------------------------------------------\n",
      "Training set size: 1002\n",
      "Test Accuracy: 0.7500\n",
      "Improvement over baseline: +0.0200\n",
      "✓ BONUS: Combined augmentation improved the baseline!\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.72      0.73      0.73        45\n",
      "        GOOD       0.78      0.76      0.77        55\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.75      0.75      0.75       100\n",
      "weighted avg       0.75      0.75      0.75       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all augmentation methods\n",
    "print(\"Creating combined augmented dataset...\")\n",
    "\n",
    "augmented_combined = []\n",
    "\n",
    "for line in tqdm(train_lines, desc=\"Combined augmentation\"):\n",
    "    label, text = parse_line(line)\n",
    "    \n",
    "    # Add original\n",
    "    augmented_combined.append((label, text))\n",
    "    \n",
    "    # Add 1 sample from each method\n",
    "    try:\n",
    "        # Mechanical\n",
    "        mech_vars = mechanical_augmentation(text, num_variants=1)\n",
    "        if mech_vars:\n",
    "            augmented_combined.append((label, mech_vars[0]))\n",
    "        \n",
    "        # Generative (lighter - just 1)\n",
    "        gen_vars = generative_augmentation(text, label, gen_tokenizer, gen_model, device, num_variants=1)\n",
    "        if gen_vars:\n",
    "            augmented_combined.append((label, gen_vars[0]))\n",
    "        \n",
    "        # Word2Vec\n",
    "        w2v_vars = word2vec_augmentation(text, w2v_model, nlp, num_variants=1)\n",
    "        if w2v_vars:\n",
    "            augmented_combined.append((label, w2v_vars[0]))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Combined augmented dataset size: {len(augmented_combined)}\")\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nExtracting features for combined dataset...\")\n",
    "X_train_combined_aug, y_train_combined_aug = extract_features_from_dataset(\n",
    "    augmented_combined, gen_tokenizer, gen_model,\n",
    "    herbert_tokenizer, herbert_model, device\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "print(f\"\\n{'Combined Augmentation (All Methods)':^70}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "scaler_comb = StandardScaler()\n",
    "X_comb_scaled = scaler_comb.fit_transform(X_train_combined_aug)\n",
    "X_test_scaled_comb = scaler_comb.transform(X_test_bert)\n",
    "\n",
    "clf_comb = LogisticRegression(random_state=42, max_iter=1000, C=1.0)\n",
    "clf_comb.fit(X_comb_scaled, y_train_combined_aug)\n",
    "\n",
    "y_test_pred_comb = clf_comb.predict(X_test_scaled_comb)\n",
    "test_acc_comb = accuracy_score(y_test, y_test_pred_comb)\n",
    "\n",
    "print(f\"Training set size: {len(X_train_combined_aug)}\")\n",
    "print(f\"Test Accuracy: {test_acc_comb:.4f}\")\n",
    "print(f\"Improvement over baseline: {test_acc_comb - bert_test_acc:+.4f}\")\n",
    "\n",
    "if test_acc_comb > bert_test_acc:\n",
    "    print(f\"✓ BONUS: Combined augmentation improved the baseline!\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_comb, target_names=['BAD', 'GOOD']))\n",
    "\n",
    "results['combined'] = test_acc_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae834fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY - TASK 3\n",
      "======================================================================\n",
      "\n",
      "Data Augmentation Results:\n",
      "----------------------------------------------------------------------\n",
      "mechanical               : 0.7500  (Δ = +0.0200) ✓ BONUS!\n",
      "word2vec                 : 0.7500  (Δ = +0.0200) ✓ BONUS!\n",
      "combined                 : 0.7500  (Δ = +0.0200) ✓ BONUS!\n",
      "baseline                 : 0.7300  (Δ = +0.0000)\n",
      "generative_model         : 0.7300  (Δ = +0.0000)\n",
      "======================================================================\n",
      "\n",
      "Task 3 Implementation Summary:\n",
      "✓ (a) Mechanical augmentation implemented (typos, case changes, Polish chars)\n",
      "✓ (b) Generative model augmentation implemented (using Papuga GPT-2)\n",
      "✓ (c) Word2Vec augmentation implemented (with morphological awareness)\n",
      "\n",
      "All three augmentation methods have been implemented and evaluated!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY - TASK 3\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nData Augmentation Results:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for method, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    improvement = acc - results['baseline'] if method != 'baseline' else 0\n",
    "    bonus = \" ✓ BONUS!\" if improvement > 0 else \"\"\n",
    "    print(f\"{method:25s}: {acc:.4f}  (Δ = {improvement:+.4f}){bonus}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTask 3 Implementation Summary:\")\n",
    "print(\"✓ (a) Mechanical augmentation implemented (typos, case changes, Polish chars)\")\n",
    "print(\"✓ (b) Generative model augmentation implemented (using Papuga GPT-2)\")\n",
    "print(\"✓ (c) Word2Vec augmentation implemented (with morphological awareness)\")\n",
    "print(\"\\nAll three augmentation methods have been implemented and evaluated!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592700c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9350e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0001ff19dd44411a9323e31d1e39f8f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "00559fa72797476aa1cc4ae81119456e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08a5f4a9285442c3b113268d54c6660f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "095d9af8a7df46e189cf991f96002bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c7f3b5010e94f5a962503a41071f430",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab768c7671d74c02a266f9bbffd2d6f3",
      "value": 288
     }
    },
    "0dc5456d00524f64af7d5d289787f9f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f36098b6b354be4be82e948bae70906",
      "placeholder": "​",
      "style": "IPY_MODEL_0001ff19dd44411a9323e31d1e39f8f1",
      "value": "Generating contextual embeddings: 100%"
     }
    },
    "12baa0ef9ed740399669a409350baa2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1586f4f61130455d8bad84ffa3080bb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f36098b6b354be4be82e948bae70906": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "360f3f750e804ced9a533673cc7091ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39307f6b31594b8d938f539ae5fba3f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aa13ee59fe2443ea6f8f4cf42b0ea66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c7f3b5010e94f5a962503a41071f430": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70db635a2ea94fbf99e8d34c99ce4109": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7250be7228a4473f8c9d73af89312ab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72766ff9a7834618b691a71274dc829c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e999cc245b2a4d8cb472f979a520079b",
       "IPY_MODEL_e78fb6e79f5a4d229f0dc5ef11dd9f71",
       "IPY_MODEL_7592a01796884cfea4f0bca0fe74ddc8"
      ],
      "layout": "IPY_MODEL_825d06bd36de4b6ea2d9b55992e168f2"
     }
    },
    "7592a01796884cfea4f0bca0fe74ddc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_360f3f750e804ced9a533673cc7091ca",
      "placeholder": "​",
      "style": "IPY_MODEL_4aa13ee59fe2443ea6f8f4cf42b0ea66",
      "value": " 288/288 [00:00&lt;00:00, 10442.61it/s]"
     }
    },
    "7a8d6d623e56447ea751c34c9fca6616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "825d06bd36de4b6ea2d9b55992e168f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92a67f5404964244b5a73f97672e178f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ec5fc4082643e690fa55d32ce4a611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a33fb0de5a5242c48eaa63808539b397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7250be7228a4473f8c9d73af89312ab5",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0ec5fc4082643e690fa55d32ce4a611",
      "value": 288
     }
    },
    "a3810f1e846b4993a3d5279d581da879": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00559fa72797476aa1cc4ae81119456e",
      "placeholder": "​",
      "style": "IPY_MODEL_7a8d6d623e56447ea751c34c9fca6616",
      "value": "100%"
     }
    },
    "ab768c7671d74c02a266f9bbffd2d6f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3b7c41c9f9b44e091272ee54750c152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92a67f5404964244b5a73f97672e178f",
      "placeholder": "​",
      "style": "IPY_MODEL_12baa0ef9ed740399669a409350baa2f",
      "value": " 288/288 [00:36&lt;00:00,  7.18it/s]"
     }
    },
    "bebd9dafb5704c02ba4efc47a85c57b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf980457a1e648feb9f806a3788d4280": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4a7778a2b114c22a190d9f5b1f87925",
      "placeholder": "​",
      "style": "IPY_MODEL_bfaab097216f4b7bac4bab49cbf32c73",
      "value": " 288/288 [00:15&lt;00:00, 20.47it/s]"
     }
    },
    "bfaab097216f4b7bac4bab49cbf32c73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4a7778a2b114c22a190d9f5b1f87925": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e612e1f136384991a8b867a67f3cd170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e78fb6e79f5a4d229f0dc5ef11dd9f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1586f4f61130455d8bad84ffa3080bb6",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08a5f4a9285442c3b113268d54c6660f",
      "value": 288
     }
    },
    "e999cc245b2a4d8cb472f979a520079b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bebd9dafb5704c02ba4efc47a85c57b2",
      "placeholder": "​",
      "style": "IPY_MODEL_e612e1f136384991a8b867a67f3cd170",
      "value": "Generating combined embeddings: 100%"
     }
    },
    "f5b94add35fb4c78a74e3070551b4386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a3810f1e846b4993a3d5279d581da879",
       "IPY_MODEL_a33fb0de5a5242c48eaa63808539b397",
       "IPY_MODEL_b3b7c41c9f9b44e091272ee54750c152"
      ],
      "layout": "IPY_MODEL_70db635a2ea94fbf99e8d34c99ce4109"
     }
    },
    "f706829fc019497c830a836f876b798c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dc5456d00524f64af7d5d289787f9f5",
       "IPY_MODEL_095d9af8a7df46e189cf991f96002bd8",
       "IPY_MODEL_bf980457a1e648feb9f806a3788d4280"
      ],
      "layout": "IPY_MODEL_39307f6b31594b8d938f539ae5fba3f2"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
