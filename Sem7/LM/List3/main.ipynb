{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065b4efd",
   "metadata": {
    "id": "065b4efd"
   },
   "source": [
    "# Zadanie 1. (4+Xp)\n",
    "\n",
    "Zajmiemy się osadzeniami słów (zarówno kontekstowymi, jak i bezkontekstowymi). Uwaga: teksty, które będziemy osadzać zawsze składają się z jednego słowa (ale niekoniecznie z jednego tokenu).\n",
    "\n",
    "**a)** Zaproponuj jakiś sposób wykorzystania bezkontekstowych osadzeń tokenów (wyznaczanych przez transformer) do wyznaczenia osadzeń słów. Możesz skorzystać z programu z wykładu 7 (`embedding.ipynb`). Sprawdź, jaką jakość (mierzoną testem ABX) mają te osadzenia. Do zaliczenia zadania wymagane jest 0.6.\n",
    "\n",
    "**b)** Wykorzystaj kontekstowe osadzenia tokenów z BERT-a do wyznaczenia osadzeń dla słów. Ponownie wykonaj testy ABX.\n",
    "\n",
    "**c)** Spróbuj połączyć te dwa podejścia w jakikolwiek sposób. Jakość twojego rozwiązania przekłada się na punkty bonusowe zgodnie z wzorem: `(score − 0.6) × 6`\n",
    "\n",
    "**Procedura ewaluacji** (być może zostanie uproszczona): osadzenia zapisz w pliku tekstowym `word_embedings_file.txt`, w którym każdy wiersz wygląda tak:\n",
    "\n",
    "```\n",
    "[słowo] float_1 float_2 ... float_D\n",
    "```\n",
    "\n",
    "Osadzenia są oceniane za pomocą skryptu `word_emb_evaluation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "edce7538",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1767361692967,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "edce7538"
   },
   "outputs": [],
   "source": [
    "clusters_txt = '''\n",
    "piśmiennicze: pisak flamaster ołówek długopis pióro\n",
    "małe_ssaki: mysz szczur chomik łasica kuna bóbr\n",
    "okręty: niszczyciel lotniskowiec trałowiec krążownik pancernik fregata korweta\n",
    "lekarze: lekarz pediatra ginekolog kardiolog internista geriatra\n",
    "zupy: rosół żurek barszcz\n",
    "uczucia: miłość przyjaźń nienawiść gniew smutek radość strach\n",
    "działy_matematyki: algebra analiza topologia logika geometria\n",
    "budynki_sakralne: kościół bazylika kaplica katedra świątynia synagoga zbór\n",
    "stopień_wojskowy: chorąży podporucznik porucznik kapitan major pułkownik generał podpułkownik\n",
    "grzyby_jadalne: pieczarka borowik gąska kurka boczniak kania\n",
    "prądy_filozoficzne: empiryzm stoicyzm racjonalizm egzystencjalizm marksizm romantyzm\n",
    "religie: chrześcijaństwo buddyzm islam prawosławie protestantyzm kalwinizm luteranizm judaizm\n",
    "dzieła_muzyczne: sonata synfonia koncert preludium fuga suita\n",
    "cyfry: jedynka dwójka trójka czwórka piątka szóstka siódemka ósemka dziewiątka\n",
    "owady: ważka biedronka żuk mrówka mucha osa pszczoła chrząszcz\n",
    "broń_biała: miecz topór sztylet nóż siekiera\n",
    "broń_palna: karabin pistolet rewolwer fuzja strzelba\n",
    "komputery: komputer laptop kalkulator notebook\n",
    "kolory: biel żółć czerwień błękit zieleń brąz czerń\n",
    "duchowny: wikary biskup ksiądz proboszcz rabin pop arcybiskup kardynał pastor\n",
    "ryby: karp śledź łosoś dorsz okoń sandacz szczupak płotka\n",
    "napoje_mleczne: jogurt kefir maślanka\n",
    "czynności_sportowe: bieganie skakanie pływanie maszerowanie marsz trucht\n",
    "ubranie:  garnitur smoking frak żakiet marynarka koszula bluzka sweter sweterek sukienka kamizelka spódnica spodnie\n",
    "mebel: krzesło fotel kanapa łóżko wersalka sofa stół stolik ława\n",
    "przestępca: morderca zabójca gwałciciel złodziej bandyta kieszonkowiec łajdak łobuz\n",
    "mięso_wędliny wieprzowina wołowina baranina cielęcina boczek baleron kiełbasa szynka schab karkówka dziczyzna\n",
    "drzewo: dąb klon wiąz jesion świerk sosna modrzew platan buk cis jawor jarzębina akacja\n",
    "źródło_światła: lampa latarka lampka żyrandol żarówka reflektor latarnia lampka\n",
    "organ: wątroba płuco serce trzustka żołądek nerka macica jajowód nasieniowód prostata śledziona\n",
    "oddziały: kompania pluton batalion brygada armia dywizja pułk\n",
    "napój_alkoholowy: piwo wino wódka dżin nalewka bimber wiśniówka cydr koniak wiśniówka\n",
    "kot_drapieżny: puma pantera lampart tygrys lew ryś żbik gepard jaguar\n",
    "metal: żelazo złoto srebro miedź nikiel cyna cynk potas platyna chrom glin aluminium\n",
    "samolot: samolot odrzutowiec awionetka bombowiec myśliwiec samolocik helikopter śmigłowiec\n",
    "owoc: jabłko gruszka śliwka brzoskwinia cytryna pomarańcza grejpfrut porzeczka nektaryna\n",
    "pościel: poduszka prześcieradło kołdra kołderka poduszeczka pierzyna koc kocyk pled\n",
    "agd: lodówka kuchenka pralka zmywarka mikser sokowirówka piec piecyk piekarnik\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9e948",
   "metadata": {
    "id": "70e9e948"
   },
   "source": [
    "task a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SgOrbbGb9-No",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4344,
     "status": "ok",
     "timestamp": 1767361740924,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "SgOrbbGb9-No",
    "outputId": "2355f5e5-41cc-4783-e457-b9d0e82bf63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Embeddings shape: (50257, 768)\n",
      "Unique words: 288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "gpt2_model_name = 'gpt2'\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name).to(device)\n",
    "\n",
    "# non-contextual token embeddings from GPT-2\n",
    "gpt2_embeddings = gpt2_model.transformer.wte.weight.detach().cpu().numpy()\n",
    "print(f\"Embeddings shape: {gpt2_embeddings.shape}\")\n",
    "\n",
    "# unique words\n",
    "words = set()\n",
    "for line in test_words.split('\\n'):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    words.update(parts[1:])\n",
    "\n",
    "print(f\"Unique words: {len(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb469d46",
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1767362275046,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "eb469d46"
   },
   "outputs": [],
   "source": [
    "def get_non_contextual_word_embedding(word, tokenizer, embeddings, method='mean'):\n",
    "    \"\"\"\n",
    "    Get non-contextual word embedding by aggregating token embeddings\n",
    "    or by isolated transformer hidden states (method='combine').\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- TOKEN-LEVEL METHODS ----------\n",
    "    tokens = tokenizer.tokenize(' ' + word)\n",
    "    if not tokens:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    if not token_ids:\n",
    "        return None\n",
    "\n",
    "    token_embeddings = embeddings[token_ids]  # [T, D]\n",
    "\n",
    "    if method == 'mean':\n",
    "        return token_embeddings.mean(axis=0)\n",
    "\n",
    "    elif method == 'first':\n",
    "        return token_embeddings[0]\n",
    "\n",
    "    elif method == 'weighted':\n",
    "        size = len(token_embeddings)\n",
    "        N = min(4, size)\n",
    "\n",
    "        weights = np.array([0.1, 0.5, 0.8, 1.][-N:])\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        vec = np.zeros(token_embeddings.shape[1])\n",
    "        for i in range(N):\n",
    "            vec += token_embeddings[size - i - 1] * weights[-i-1]\n",
    "\n",
    "        return vec\n",
    "\n",
    "    elif method == 'positional':\n",
    "        D = token_embeddings.shape[1]\n",
    "        pos = np.arange(len(token_embeddings))\n",
    "        pos_enc = np.sin(pos[:, None] / (10000 ** (2 * np.arange(D) / D)))\n",
    "        return (token_embeddings * (1 + 0.1 * pos_enc)).mean(axis=0)\n",
    "\n",
    "    # ---------- TRANSFORMER-LEVEL METHODS ----------\n",
    "    elif method == 'combine':\n",
    "        gpt2_model.eval()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            ' ' + word,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=False\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model.transformer(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "        # hidden_states: tuple[L] of [1, T, D]\n",
    "        hs = torch.stack(outputs.hidden_states).squeeze(1)  # [L, T, D]\n",
    "\n",
    "        # mean of last 4 layers\n",
    "        last4 = hs[-4:].mean(dim=0)                          # [T, D]\n",
    "\n",
    "        vec = last4[-1].cpu().numpy()\n",
    "        return vec / (np.linalg.norm(vec) + 1e-9)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "e65c8cef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "f5b94add35fb4c78a74e3070551b4386",
      "a3810f1e846b4993a3d5279d581da879",
      "a33fb0de5a5242c48eaa63808539b397",
      "b3b7c41c9f9b44e091272ee54750c152",
      "70db635a2ea94fbf99e8d34c99ce4109",
      "00559fa72797476aa1cc4ae81119456e",
      "7a8d6d623e56447ea751c34c9fca6616",
      "7250be7228a4473f8c9d73af89312ab5",
      "a0ec5fc4082643e690fa55d32ce4a611",
      "92a67f5404964244b5a73f97672e178f",
      "12baa0ef9ed740399669a409350baa2f"
     ]
    },
    "executionInfo": {
     "elapsed": 36840,
     "status": "ok",
     "timestamp": 1767365094302,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "e65c8cef",
    "outputId": "f9781541-3149-4d61-a7d6-2fe72ac2f2ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b94add35fb4c78a74e3070551b4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 288/288 words\n"
     ]
    }
   ],
   "source": [
    "embeddings_1a = {}\n",
    "for word in tqdm(words):\n",
    "    emb = get_non_contextual_word_embedding(word, gpt2_tokenizer, gpt2_embeddings, method='combine')\n",
    "    if emb is not None:\n",
    "        embeddings_1a[word] = emb\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings_1a)}/{len(words)} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4e28b533",
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1767365094355,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "4e28b533"
   },
   "outputs": [],
   "source": [
    "X = np.stack(list(embeddings_1a.values()))\n",
    "mu = X.mean(axis=0)\n",
    "\n",
    "# subtract + normalize\n",
    "for w in embeddings_1a:\n",
    "    v = embeddings_1a[w] - mu\n",
    "    embeddings_1a[w] = v / (np.linalg.norm(v) + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "66260fc6",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767365094356,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "66260fc6"
   },
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings_dict, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for word, emb in embeddings_dict.items():\n",
    "            # format: word float1 float2 ... floatD\n",
    "            emb_str = ' '.join(map(str, emb))\n",
    "            f.write(f\"{word} {emb_str}\\n\")\n",
    "    print(f\"Saved {len(embeddings_dict)} embeddings to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4d5fe161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1767365094482,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "4d5fe161",
    "outputId": "3f86985e-44fd-480d-c67c-3fe5062508b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 288 embeddings to word_embedings_file.txt\n",
      "Embeddings saved\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(embeddings_1a, 'word_embedings_file.txt')\n",
    "print(\"Embeddings saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "DrsKiiEFt-yw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3523,
     "status": "ok",
     "timestamp": 1767365098006,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "DrsKiiEFt-yw",
    "outputId": "8b170811-0bd0-499c-9d45-b2fc543de2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.61432\n"
     ]
    }
   ],
   "source": [
    "!python3 word_emb_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781769bc",
   "metadata": {
    "id": "781769bc"
   },
   "source": [
    "task b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7693f9c7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1767365100580,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "7693f9c7"
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'distilbert-base-multilingual-cased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name, output_hidden_states=True).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_contextual_word_embedding(word, tokenizer, model, device, layer=-1):\n",
    "    \"\"\"\n",
    "    Get contextual word embedding from BERT.\n",
    "    Places word in minimal context to get contextual representation.\n",
    "    \"\"\"\n",
    "    text = f\"To jest {word}.\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # get embeddings from specified layer\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    layer_embedding = hidden_states[layer]\n",
    "\n",
    "    # find the tokens that correspond to the word\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    word_token_indices = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in ['[CLS]', '[PAD]', '[SEP]']:\n",
    "            continue\n",
    "\n",
    "        # check if this could be the start of our word\n",
    "        match = True\n",
    "        for j, wt in enumerate(word_tokens):\n",
    "            if i + j >= len(tokens) or tokens[i + j] != wt:\n",
    "                match = False\n",
    "                break\n",
    "        if match:\n",
    "            word_token_indices = list(range(i, i + len(word_tokens)))\n",
    "            break\n",
    "\n",
    "    # if we couldn't find the word, just take the middle tokens (skip [CLS])\n",
    "    if not word_token_indices:\n",
    "        word_token_indices = list(range(1, len(tokens) - 1))\n",
    "\n",
    "    # average embeddings of word tokens\n",
    "    if word_token_indices:\n",
    "        word_embedding = layer_embedding[0, word_token_indices, :].mean(dim=0).cpu().numpy()\n",
    "        return word_embedding\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "9ca98b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "f706829fc019497c830a836f876b798c",
      "0dc5456d00524f64af7d5d289787f9f5",
      "095d9af8a7df46e189cf991f96002bd8",
      "bf980457a1e648feb9f806a3788d4280",
      "39307f6b31594b8d938f539ae5fba3f2",
      "1f36098b6b354be4be82e948bae70906",
      "0001ff19dd44411a9323e31d1e39f8f1",
      "6c7f3b5010e94f5a962503a41071f430",
      "ab768c7671d74c02a266f9bbffd2d6f3",
      "e4a7778a2b114c22a190d9f5b1f87925",
      "bfaab097216f4b7bac4bab49cbf32c73"
     ]
    },
    "executionInfo": {
     "elapsed": 15646,
     "status": "ok",
     "timestamp": 1767365116245,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "9ca98b28",
    "outputId": "7403a1a2-b92b-48b9-90d5-925588c731d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f706829fc019497c830a836f876b798c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating contextual embeddings:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated embeddings for 288/288 words\n",
      "Saved 288 embeddings to word_embedings_file.txt\n",
      "Embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "embeddings_1b = {}\n",
    "for word in tqdm(words, desc=\"Generating contextual embeddings\"):\n",
    "    emb = get_contextual_word_embedding(word, bert_tokenizer, bert_model, device)\n",
    "    if emb is not None:\n",
    "        # normalize embedding\n",
    "        emb = emb / np.linalg.norm(emb)\n",
    "        embeddings_1b[word] = emb\n",
    "\n",
    "print(f\"Successfully generated embeddings for {len(embeddings_1b)}/{len(words)} words\")\n",
    "\n",
    "save_embeddings(embeddings_1b, 'word_embedings_file.txt')\n",
    "print(\"Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "89ojESWnuOKj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4835,
     "status": "ok",
     "timestamp": 1767365121081,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "89ojESWnuOKj",
    "outputId": "407be987-7e29-489b-a84c-7d182dcb30f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.688636\n"
     ]
    }
   ],
   "source": [
    "!python3 word_emb_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27a4f0",
   "metadata": {
    "id": "3e27a4f0"
   },
   "source": [
    "task c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "8ab87088",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767365121083,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "8ab87088"
   },
   "outputs": [],
   "source": [
    "def combine_embeddings(emb_noncontextual, emb_contextual, method='weighted', weight_contextual=0.5):\n",
    "    \"\"\"\n",
    "    Combine non-contextual and contextual embeddings\n",
    "    Methods: 'weighted', 'concat'\n",
    "    \"\"\"\n",
    "    if emb_noncontextual is None or emb_contextual is None:\n",
    "        return None\n",
    "\n",
    "    # normalize\n",
    "    emb_nc = emb_noncontextual / np.linalg.norm(emb_noncontextual)\n",
    "    emb_c = emb_contextual / np.linalg.norm(emb_contextual)\n",
    "\n",
    "    if method == 'weighted':\n",
    "        # weighted average\n",
    "        combined = (1 - weight_contextual) * emb_nc + weight_contextual * emb_c\n",
    "        return combined / np.linalg.norm(combined)\n",
    "    elif method == 'concat':\n",
    "        # concatenation\n",
    "        combined = np.concatenate([emb_nc, emb_c])\n",
    "        return combined / np.linalg.norm(combined)\n",
    "    else:\n",
    "        return emb_nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3e079828",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "72766ff9a7834618b691a71274dc829c",
      "e999cc245b2a4d8cb472f979a520079b",
      "e78fb6e79f5a4d229f0dc5ef11dd9f71",
      "7592a01796884cfea4f0bca0fe74ddc8",
      "825d06bd36de4b6ea2d9b55992e168f2",
      "bebd9dafb5704c02ba4efc47a85c57b2",
      "e612e1f136384991a8b867a67f3cd170",
      "1586f4f61130455d8bad84ffa3080bb6",
      "08a5f4a9285442c3b113268d54c6660f",
      "360f3f750e804ced9a533673cc7091ca",
      "4aa13ee59fe2443ea6f8f4cf42b0ea66"
     ]
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1767365121286,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "3e079828",
    "outputId": "960ee4cb-35df-41ad-eb98-18a503cbe89b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72766ff9a7834618b691a71274dc829c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating combined embeddings:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated combined embeddings for 288/288 words\n",
      "Saved 288 embeddings to word_embedings_file.txt\n",
      "Embeddings saved\n"
     ]
    }
   ],
   "source": [
    "embeddings_1c = {}\n",
    "for word in tqdm(words, desc=\"Generating combined embeddings\"):\n",
    "    if word in embeddings_1a and word in embeddings_1b:\n",
    "        combined_emb = combine_embeddings(embeddings_1a[word], embeddings_1b[word], method='weighted', weight_contextual=0.5)\n",
    "        if combined_emb is not None:\n",
    "            embeddings_1c[word] = combined_emb\n",
    "\n",
    "print(f\"Successfully generated combined embeddings for {len(embeddings_1c)}/{len(words)} words\")\n",
    "\n",
    "\n",
    "save_embeddings(embeddings_1c, 'word_embedings_file.txt')\n",
    "print(\"Embeddings saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "HSFp2cLTuO5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1767365124918,
     "user": {
      "displayName": "Patryk Flama",
      "userId": "14474424760527076284"
     },
     "user_tz": -60
    },
    "id": "HSFp2cLTuO5e",
    "outputId": "772cebbd-c8b7-4d96-b164-4686e3a20d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.662298\n"
     ]
    }
   ],
   "source": [
    "!python3 word_emb_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a381e6a",
   "metadata": {
    "id": "8a381e6a"
   },
   "source": [
    "# Zadanie 2. (6+Xp)\n",
    "\n",
    "W zadaniu tym będziemy zajmować się klasyfikacją recenzji z wykorzystaniem modeli transformer, możesz tu skorzystać z programu z wykładu (`herbert.ipynb`). W tym zadaniu powinieneś użyć trzech modeli:\n",
    "\n",
    "1. Modelu generatywnego, takiego jak Papuga, Polka o wielkości do 1B, który znajduje prawdopodobieństwa tekstu (podobnie, jak na liście 1)\n",
    "2. Kodera typu BERT (np. herbert), jako ekstraktora cech\n",
    "3. Tradycyjnego modelu Machine Learning, który integruje wyniki dwóch poprzednich modeli.\n",
    "\n",
    "Ten model powinieneś wytrenować na zbiorze treningowym recenzji, a testować na testowym.\n",
    "\n",
    "Wartość premii jest równa: `20 × (a − 0.85)`, gdzie `a` to wartość accuracy na zbiorze testowym.\n",
    "\n",
    "Jeżeli chcesz, możesz skorzystać tu również z wyników kolejnego zadania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb0061",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "808b01c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "bert_model_name = \"allegro/herbert-base-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "gen_model_name = 'flax-community/papuGaPT2'\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
    "gen_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6bef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8160b63bc7c34fb3bc7afaa9ee09ca11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training set:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeefdc3c3df4388a98be15ab860002f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9991666666666666\n",
      "Test accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "lines = open('reviews_for_task3.txt').readlines()\n",
    "\n",
    "def representation(L):\n",
    "    txt = ' '.join(L)\n",
    "\n",
    "    bert_input_ids = bert_tokenizer(txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    bert_output = bert_model(input_ids=bert_input_ids)\n",
    "    bert_vec = bert_output.last_hidden_state.detach().cpu().numpy()[0,0,:]\n",
    "    \n",
    "    gen_input_ids = gen_tokenizer(txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    gen_output = gen_model(input_ids=gen_input_ids, output_hidden_states=True)\n",
    "    gen_hidden = gen_output.hidden_states[-1]\n",
    "    gen_vec = gen_hidden.detach().cpu().numpy()[0,0,:]\n",
    "\n",
    "    return np.concatenate([bert_vec, gen_vec])\n",
    "\n",
    "\n",
    "def spoil(L):\n",
    "    res = []\n",
    "    for w in L:\n",
    "        if random.random() < 0.85:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            res.append(w.upper())\n",
    "    return res\n",
    "\n",
    "# random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "N = len(lines)\n",
    "test_size = N // 4\n",
    "train_size = N - test_size\n",
    "\n",
    "train_lines = lines[:train_size]\n",
    "test_lines = lines[train_size:]\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for line in tqdm(train_lines, desc=\"Processing training set\"):\n",
    "    L = line.split()\n",
    "    y = 0 if L[0] == 'BAD' else 1\n",
    "    \n",
    "    x = representation(L[1:])\n",
    "    y_train.append(y)\n",
    "    X_train.append(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = representation(spoil(L[1:]))\n",
    "        y_train.append(y)\n",
    "        X_train.append(x)\n",
    "    \n",
    "for line in tqdm(test_lines, desc=\"Processing test set\"):\n",
    "    L = line.split()\n",
    "    y = 0 if L[0] == 'BAD' else 1\n",
    "    \n",
    "    x = representation(L[1:])\n",
    "    y_test.append(y)\n",
    "    X_test.append(x)\n",
    "        \n",
    "\n",
    "N = len(lines)\n",
    "test_size = N // 4\n",
    "train_size = N - test_size\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm \n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print ('Train accuracy:', clf.score(X_train, y_train))\n",
    "print ('Test accuracy:', clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64deccc",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6749d2e5",
   "metadata": {
    "id": "6749d2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reviews: 400\n",
      "Training: 300\n",
      "Test: 100\n",
      "\n",
      "Sample review:\n",
      "Label: GOOD\n",
      "Text: Jedzenie doskonale.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews_file = 'reviews_for_task3.txt'\n",
    "lines = open(reviews_file).readlines()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "# preprocess data\n",
    "def parse_line(line):\n",
    "    parts = line.strip().split(maxsplit=1)\n",
    "    label = 1 if parts[0] == 'GOOD' else 0\n",
    "    text = parts[1] if len(parts) > 1 else \"\"\n",
    "    return label, text\n",
    "\n",
    "# stratified split to keep label balance\n",
    "labels = [parse_line(l)[0] for l in lines]\n",
    "train_lines, test_lines = train_test_split(\n",
    "    lines,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "N = len(lines)\n",
    "print(f\"All reviews: {N}\")\n",
    "print(f\"Training: {len(train_lines)}\")\n",
    "print(f\"Test: {len(test_lines)}\")\n",
    "\n",
    "sample_label, sample_text = parse_line(train_lines[0])\n",
    "print(f\"\\nSample review:\")\n",
    "print(f\"Label: {'GOOD' if sample_label == 1 else 'BAD'}\")\n",
    "print(f\"Text: {sample_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80044a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Loaded model\n",
      "Sample generative features: [-6.809865   -6.288423   -0.52144194  0.3725151 ]\n",
      "Loaded model\n",
      "Sample generative features: [-6.809865   -6.288423   -0.52144194  0.3725151 ]\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Papuga\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "gen_model_name = 'flax-community/papuGaPT2'\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
    "gen_model.eval()\n",
    "\n",
    "print(f\"Loaded model\")\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def avg_token_log_prob(sentence_txt, tokenizer, model):\n",
    "    input_ids = tokenizer(sentence_txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids)\n",
    "        # shift for next-token prediction\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], input_ids[:, 1:])\n",
    "        seq_log_probs = torch.sum(log_probs)\n",
    "        seq_len = input_ids.size(1) - 1\n",
    "    # average per-token log-probability (length-normalized)\n",
    "    return (seq_log_probs / max(seq_len, 1)).item()\n",
    "\n",
    "def papuga_get_opinion_features(sentence_txt, tokenizer, model):\n",
    "    good_text = \" To jest opinia pozytywna: \"\n",
    "    bad_text = \" To jest opinia negatywna: \"\n",
    "\n",
    "    sent_good = sentence_txt + good_text\n",
    "    sent_bad = sentence_txt + bad_text\n",
    "\n",
    "    avg_logp_good = avg_token_log_prob(sent_good, tokenizer, model)\n",
    "    avg_logp_bad = avg_token_log_prob(sent_bad, tokenizer, model)\n",
    "\n",
    "    # difference and softmax-like probability between the two prompts\n",
    "    diff = avg_logp_good - avg_logp_bad\n",
    "    p_good = np.exp(avg_logp_good) / (np.exp(avg_logp_good) + np.exp(avg_logp_bad))\n",
    "\n",
    "    # Return a compact, informative feature vector\n",
    "    return np.array([avg_logp_good, avg_logp_bad, diff, p_good], dtype=np.float32)\n",
    "\n",
    "sample_features = papuga_get_opinion_features(sample_text, gen_tokenizer, gen_model)\n",
    "print(f\"Sample generative features: {sample_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e004d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /shared/programming/UWr/Sem7/LM/.venv/lib/python3.12/site-packages (from sacremoses) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "084b2b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "BERT feature shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Model 2: BERT (Herbert)\n",
    "from transformers import AutoModel\n",
    "\n",
    "herbert_model_name = 'allegro/herbert-base-cased'\n",
    "herbert_tokenizer = AutoTokenizer.from_pretrained(herbert_model_name)\n",
    "herbert_model = AutoModel.from_pretrained(herbert_model_name).to(device)\n",
    "herbert_model.eval()\n",
    "\n",
    "print(f\"Loaded model\")\n",
    "\n",
    "def get_bert_features(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state[0]  # [seq_len, hidden]\n",
    "        mask = inputs['attention_mask'][0].unsqueeze(-1).float()  # [seq_len, 1]\n",
    "        # mean-pooling over valid tokens (often stronger than CLS)\n",
    "        pooled = (last_hidden * mask).sum(dim=0) / mask.sum()\n",
    "\n",
    "    return pooled.cpu().numpy()\n",
    "\n",
    "\n",
    "sample_bert_features = get_bert_features(sample_text, herbert_tokenizer, herbert_model)\n",
    "print(f\"BERT feature shape: {sample_bert_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca224dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8425e5be41348f48e41957a3c57ba61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training features:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set feature shapes:\n",
      "  Generative features: (1200, 4)\n",
      "  BERT features: (1200, 768)\n",
      "  Combined features: (1200, 772)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def spoil(L):\n",
    "    res = []\n",
    "    for w in L:\n",
    "        if random.random() < 0.85:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            res.append(w.upper())\n",
    "    return res\n",
    "\n",
    "X_train_gen = []\n",
    "X_train_bert = []\n",
    "y_train = []\n",
    "\n",
    "for i, line in enumerate(tqdm(train_lines, desc=\"Training features\")):\n",
    "    label, text = parse_line(line)\n",
    "\n",
    "    # original\n",
    "    gen_features = papuga_get_opinion_features(text, gen_tokenizer, gen_model)\n",
    "    X_train_gen.append(gen_features)\n",
    "\n",
    "    bert_features = get_bert_features(text, herbert_tokenizer, herbert_model)\n",
    "    X_train_bert.append(bert_features)\n",
    "    y_train.append(label)\n",
    "\n",
    "    # add spoils\n",
    "    for _ in range(3):\n",
    "        text_spoiled = ' '.join(spoil(text.split()))\n",
    "        gen_features_sp = papuga_get_opinion_features(text_spoiled, gen_tokenizer, gen_model)\n",
    "        X_train_gen.append(gen_features_sp)\n",
    "\n",
    "        bert_features_sp = get_bert_features(text_spoiled, herbert_tokenizer, herbert_model)\n",
    "        X_train_bert.append(bert_features_sp)\n",
    "        y_train.append(label)\n",
    "\n",
    "X_train_gen = np.array(X_train_gen)\n",
    "X_train_bert = np.array(X_train_bert)\n",
    "X_train_combined = np.concatenate([X_train_gen, X_train_bert], axis=1)\n",
    "\n",
    "print(f\"\\nTraining set feature shapes:\")\n",
    "print(f\"  Generative features: {X_train_gen.shape}\")\n",
    "print(f\"  BERT features: {X_train_bert.shape}\")\n",
    "print(f\"  Combined features: {X_train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcef42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce1b4f26d454fb0b03c516943e1bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test features:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set feature shapes:\n",
      "  Generative features: (100, 4)\n",
      "  BERT features: (100, 768)\n",
      "  Combined features: (100, 772)\n"
     ]
    }
   ],
   "source": [
    "X_test_gen = []\n",
    "X_test_bert = []\n",
    "y_test = []\n",
    "\n",
    "for line in tqdm(test_lines, desc=\"Test features\"):\n",
    "    label, text = parse_line(line)\n",
    "\n",
    "    # get generative model features\n",
    "    gen_features = papuga_get_opinion_features(text, gen_tokenizer, gen_model)\n",
    "    X_test_gen.append(gen_features)\n",
    "    \n",
    "    # get BERT features\n",
    "    bert_features = get_bert_features(text, herbert_tokenizer, herbert_model)\n",
    "    X_test_bert.append(bert_features)\n",
    "    \n",
    "    y_test.append(label)\n",
    "\n",
    "# combine features\n",
    "X_test_gen = np.array(X_test_gen)\n",
    "X_test_bert = np.array(X_test_bert)\n",
    "X_test_combined = np.concatenate([X_test_gen, X_test_bert], axis=1)\n",
    "\n",
    "print(f\"\\nTest set feature shapes:\")\n",
    "print(f\"  Generative features: {X_test_gen.shape}\")\n",
    "print(f\"  BERT features: {X_test_bert.shape}\")\n",
    "print(f\"  Combined features: {X_test_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb681206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS - Combined Model (Generative + BERT + LogReg)\n",
      "==================================================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.8200\n",
      "\n",
      "Bonus points: -0.60p\n",
      "==================================================\n",
      "\n",
      "Detailed Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAD       0.81      0.84      0.82        50\n",
      "        GOOD       0.83      0.80      0.82        50\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.82      0.82      0.82       100\n",
      "weighted avg       0.82      0.82      0.82       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Train Logistic Regression on combined features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "X_test_scaled = scaler.transform(X_test_combined)\n",
    "\n",
    "# logistic regression\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, C=0.5, class_weight='balanced')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predictions\n",
    "y_train_pred = clf.predict(X_train_scaled)\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS - Combined Model (Generative + BERT + LogReg)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"\\nBonus points: {20 * (test_accuracy - 0.85):.2f}p\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\nDetailed Test Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['BAD', 'GOOD']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33e7d2",
   "metadata": {
    "id": "9d33e7d2"
   },
   "source": [
    "# Zadanie 3. (8+1p)\n",
    "\n",
    "W tym zadaniu powinieneś sprawdzić, czy augmentacja danych może poprawić wyniki klasyfikacji, w której BERT jest traktowany jako ekstraktor cech. Mamy 3 osobno punktowane procedury generowania nowych wariantów recenzji:\n",
    "\n",
    "**a)** Augmentacja mechaniczna (czyli wprowadzasz jakieś zniekształcenia w tekście, mogą to być na przykład literówki, zmiana wielkości liter, błędy związane z polskimi literami, etc). **(2p)**\n",
    "\n",
    "**b)** Augmentacja modelem generatywnym, na przykład Papugą. Powinieneś generować recenzje, które bazują na oryginalnej recenzji, zachowując jej polarność (czyli to, czy jest pozytywna, czy negatywna). Zwróć uwagę, że „fantazja\" modelu językowego nie musi tu być wadą – tak naprawdę to niekoniecznie w tej procedurze muszą powstawać poprawne teksty. **(3p)**\n",
    "\n",
    "**c)** Ta procedura augmentacji powinna bazować na Word2Vec i zachowywać w miarę możliwości znaczenie tekstu. Należy wybrane słowa zamieniać na słowa bliskoznaczne, w tej samej formie gramatycznej (będzie to dokładniej omówione na kolejnym wykładzie). **(3p)**\n",
    "\n",
    "Przykładowo recenzja:\n",
    "- *Hotel ogólnie bardzo ładny.* mogłaby być zmieniona na *Pensjonat szczególnie bardzo piękny.*\n",
    "- *Polecam wszystkim tego fizjoterapeutę!* na *Rekomenduję wszystkim tego ortopedę!*\n",
    "\n",
    "Konieczne informacje gramatyczne pojawią się na wykładzie 8 (czyli najbliższym).\n",
    "\n",
    "---\n",
    "\n",
    "Każda recenzja powinna posłużyć do wygenerowania K innych recenzji (dobór K to Twoje zadanie), stąd należy generator napisać w ten sposób, by recenzje były tworzone niedeterministycznie.\n",
    "\n",
    "Dla wybranych (lub wszystkich) procedur przeprowadź uczenie na zaugmentowanych danych za pomocą regresji logistycznej. Dodatkowo można uzyskać **1p premii**, jeżeli któraś z procedur da korzyść w porównaniu do oryginalnych danych (tzn. dzięki augmentacji uda się uzyskać lepszy wynik dla danych testowych).\n",
    "\n",
    "W zadaniu do maksimum wlicza się **6p**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed3b4e",
   "metadata": {},
   "source": [
    "## Task 3a - Mechanical Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9be0e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hotel ogólnie bardzo ładny. Polecam wszystkim tego fizjoterapeutę!\n",
      "\n",
      "Mechanical augmentations:\n",
      "1. Hotel OGólNie bardzo ładny. Polecam  Wszystkim tego fizjoterapeutę!\n",
      "2. Hotel Ogólnie bardzo ładny. Polecam wszystkim tego fizojterapeutę!\n",
      "3. Hotel ogólnie bardzo ładny. Polecam wszystkim Tego ffizjterapeutę!\n",
      "4. Hotel ogólnie bardzo ładny. PolecaM Wszystkim Yego fizjoterapeutę!\n",
      "5. HOtel ogólnIe bardzo ładny. Polecam wszystkim tego fizjoterapeutę!\n",
      "6. Hotel ogólnie bardzo ładny. Polecam wszystKim tego fizjoterapeutę!\n",
      "7. Hotel ogólni ebardzO ładny. Polecam wszystkim tego fizjoterapeutę!\n",
      "8. Hotel ogólnie bardzo ładny. Polecam wszystkim tego FizjoterpeutĘ!\n",
      "9. Hotel ogólnie bardzo ładny. Polecam wszystkim tego fizjoterapeutĘ!\n",
      "10. Hotel ogólnie bardzo ładny. Polecam Wszystkim tego fizjoterapeute!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def mechanical_augmentation(text, num_variants=3):\n",
    "    variants = []\n",
    "    \n",
    "    # de-polish'ify\n",
    "    polish_chars = {\n",
    "        'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l', \n",
    "        'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z',\n",
    "        'Ą': 'A', 'Ć': 'C', 'Ę': 'E', 'Ł': 'L',\n",
    "        'Ń': 'N', 'Ó': 'O', 'Ś': 'S', 'Ź': 'Z', 'Ż': 'Z'\n",
    "    }\n",
    "    \n",
    "    # for i in range(num_variants):\n",
    "    while len(variants) < num_variants:\n",
    "        chars = list(text)\n",
    "\n",
    "        num_changes = random.randint(2, max(2, len(text) // 10))\n",
    "\n",
    "        for _ in range(num_changes):\n",
    "            transform_type = random.choices(\n",
    "                ['case', 'polish', 'typo', 'space'],\n",
    "                weights=[0.3, 0.3, 0.1, 0.2],\n",
    "                k=1\n",
    "            )[0]\n",
    "\n",
    "            # if len(chars) < 3:\n",
    "            #     break\n",
    "\n",
    "            pos = random.randint(0, len(chars) - 1)\n",
    "\n",
    "            # case change\n",
    "            if transform_type == 'case' and chars[pos].isalpha():\n",
    "                chars[pos] = chars[pos].swapcase()\n",
    "\n",
    "            # de-polish'ify\n",
    "            elif transform_type == 'polish' and chars[pos] in polish_chars:\n",
    "                chars[pos] = polish_chars[chars[pos]]\n",
    "        \n",
    "            # typo\n",
    "            elif transform_type == 'typo' and chars[pos].isalpha():\n",
    "                typo_type = random.choice(['swap', 'subs', 'delete', 'duplicate'])\n",
    "        \n",
    "                # swap adjacent characters\n",
    "                if typo_type == 'swap' and pos < len(chars) - 1:\n",
    "                    chars[pos], chars[pos + 1] = chars[pos + 1], chars[pos]\n",
    "            \n",
    "                # substitute character\n",
    "                elif typo_type == 'subs':\n",
    "                    substitute_char = random.randint(ord('a'), ord('z'))\n",
    "\n",
    "                    if chars[pos].isupper():\n",
    "                        substitute_char = chr(substitute_char).upper()\n",
    "                    else:\n",
    "                        substitute_char = chr(substitute_char)\n",
    "\n",
    "                    chars[pos] = substitute_char\n",
    "\n",
    "                # delete character\n",
    "                elif typo_type == 'delete':\n",
    "                    chars.pop(pos)\n",
    "            \n",
    "                # duplicate character\n",
    "                elif typo_type == 'duplicate':\n",
    "                    chars.insert(pos, chars[pos])\n",
    "            \n",
    "            # remove or add space\n",
    "            elif transform_type == 'space' and chars[pos] == ' ':\n",
    "                if random.random() > 0.5:\n",
    "                    chars.pop(pos)\n",
    "                else:\n",
    "                    chars.insert(pos, ' ')\n",
    "\n",
    "        augmented = ''.join(chars)\n",
    "\n",
    "        if augmented and augmented != text:\n",
    "            variants.append(augmented)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "sample_text = \"Hotel ogólnie bardzo ładny. Polecam wszystkim tego fizjoterapeutę!\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"\\nMechanical augmentations:\")\n",
    "for i, variant in enumerate(mechanical_augmentation(sample_text, 10), 1):\n",
    "    print(f\"{i}. {variant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1f186f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mechanical_augmentation_univ(text, label, num_variants=2):\n",
    "    return mechanical_augmentation(text, num_variants=num_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd08740",
   "metadata": {},
   "source": [
    "## Task 3b - Generative Model Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a0e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3654fde16f53451395ab29936d6e019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/208 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d02dfcbfaf4372a35f6c3d17965886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/888k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d50f974a3743b09919a125c8e31826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/547k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a39505f8b84fce9f6140308e91f7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3977d794f0493e83221b1a7491f093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e029e714d94e4ce6b794a7271bf3c1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be4810751d74364a148fa5933efe300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/864 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da224cfddcc40c190a3c083dcb4e32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "gen_model_name = 'flax-community/papuGaPT2'\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
    "gen_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc277a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews_file = 'reviews_for_task3.txt'\n",
    "lines = open(reviews_file).readlines()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "# preprocess data\n",
    "def parse_line(line):\n",
    "    parts = line.strip().split(maxsplit=1)\n",
    "    label = 1 if parts[0] == 'GOOD' else 0\n",
    "    text = parts[1] if len(parts) > 1 else \"\"\n",
    "    return label, text\n",
    "\n",
    "# stratified split to keep label balance\n",
    "labels = [parse_line(l)[0] for l in lines]\n",
    "train_lines, test_lines = train_test_split(\n",
    "    lines,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9eb8f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review:\n",
      "Label: GOOD\n",
      "Text: Jedzenie doskonale.\n",
      "\n",
      "Generative augmentations:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce224aaade9464d869dc5a7c45b8a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Recenzja pozytywna: Jedzenie doskonale.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generative_augmentation(text, label, tokenizer, model, device, num_variants=3, max_length=100):\n",
    "    variants = []\n",
    "\n",
    "    # prompts based on label\n",
    "    sentiment = \"pozytywna\" if label == 1 else \"negatywna\"\n",
    "\n",
    "    # random prompt templates\n",
    "    templates = [\n",
    "        f\"{text[:30]}\",\n",
    "        f\"Recenzja {sentiment}: {text[:20]}\",\n",
    "    ]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for template in templates[:num_variants]:\n",
    "        inputs = tokenizer(template, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_length=min(max_length, len(inputs['input_ids'][0]) + 50),\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=random.uniform(0.8, 1.2),\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        generated = generated.strip()\n",
    "        generated = re.split(r'(?<=[.!?]) +', generated)[0]     # take first sentence\n",
    "        if generated and generated != text and len(generated) > 10:\n",
    "            variants.append(generated)\n",
    "\n",
    "    return variants\n",
    "\n",
    "\n",
    "sample_label, sample_text = parse_line(train_lines[0])\n",
    "print(\"Original review:\")\n",
    "print(f\"Label: {'GOOD' if sample_label == 1 else 'BAD'}\")\n",
    "print(f\"Text: {sample_text}\\n\")\n",
    "\n",
    "print(\"Generative augmentations:\")\n",
    "gen_variants = generative_augmentation(sample_text, sample_label, gen_tokenizer, gen_model, device, num_variants=3)\n",
    "for i, variant in enumerate(gen_variants, 1):\n",
    "    print(f\"{i}. {variant}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ea5994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_augmentation_univ(text, label, num_variants=3):\n",
    "    return generative_augmentation(text, label, gen_tokenizer, gen_model, device, num_variants=num_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec657e49",
   "metadata": {},
   "source": [
    "## Task 3c - Word2Vec Augmentation with Grammatical Form Preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd829286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n",
      "Successfully installed gensim-4.4.0\n",
      "Collecting pl-core-news-sm==3.8.0\n",
      "Collecting pl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.8.0/pl_core_news_sm-3.8.0-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/20.2 MB\u001b[0m \u001b[31m252.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.8.0/pl_core_news_sm-3.8.0-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pl-core-news-sm\n",
      "Successfully installed pl-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_sm')\n",
      "Installing collected packages: pl-core-news-sm\n",
      "Successfully installed pl-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim spacy\n",
    "!python -m spacy download pl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9de41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Polish Word2Vec model...\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Loaded English Word2Vec (fallback)\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Loaded English Word2Vec (fallback)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Polish spaCy model for morphological analysis\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "# Download Polish Word2Vec model\n",
    "print(\"Downloading Polish Word2Vec model...\")\n",
    "# Using a Polish word2vec model - this might take a while\n",
    "# Alternative: train on the review corpus itself\n",
    "try:\n",
    "    word2vec_model = api.load('word2vec-google-news-300')  # English fallback\n",
    "    print(\"Loaded English Word2Vec (fallback)\")\n",
    "except:\n",
    "    print(\"Could not load Word2Vec model\")\n",
    "    word2vec_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2c047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec on 300 reviews...\n",
      "Vocabulary size: 453\n",
      "\n",
      "Original review:\n",
      "Label: GOOD\n",
      "Text: Jedzenie doskonale.\n",
      "\n",
      "Word2Vec augmentations:\n",
      "1. Jedzenie wybrać.\n",
      "2. każdy doskonale.\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec on our review corpus for better Polish support\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare corpus from reviews\n",
    "corpus = []\n",
    "for line in train_lines:\n",
    "    _, text = parse_line(line)\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    corpus.append(tokens)\n",
    "\n",
    "print(f\"Training Word2Vec on {len(corpus)} reviews...\")\n",
    "w2v_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")\n",
    "\n",
    "def find_similar_word_with_form(word, doc_token, w2v_model, top_n=10):\n",
    "    \"\"\"\n",
    "    Find similar word and try to match grammatical form.\n",
    "    Uses lemma for similarity search, then tries to match morphological tags.\n",
    "    \"\"\"\n",
    "    lemma = doc_token.lemma_.lower()\n",
    "    \n",
    "    # Get similar words based on lemma\n",
    "    try:\n",
    "        similar_words = w2v_model.wv.most_similar(lemma, topn=top_n)\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "    # Get morphological features\n",
    "    original_pos = doc_token.pos_\n",
    "    original_morph = doc_token.morph\n",
    "    \n",
    "    # Try to find a similar word with matching POS\n",
    "    for similar_word, similarity in similar_words:\n",
    "        # Skip if too similar (same word)\n",
    "        if similar_word == lemma:\n",
    "            continue\n",
    "            \n",
    "        # Simple heuristic: use the similar word as-is\n",
    "        # In production, you'd use a morphological generator here\n",
    "        return similar_word\n",
    "    \n",
    "    return None\n",
    "\n",
    "def word2vec_augmentation(text, w2v_model, nlp, num_variants=3, replacement_prob=0.3):\n",
    "    \"\"\"\n",
    "    Generate augmented versions using Word2Vec synonym replacement.\n",
    "    Attempts to preserve grammatical form.\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for _ in range(num_variants):\n",
    "        new_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation and spaces\n",
    "            if token.is_punct or token.is_space:\n",
    "                new_tokens.append(token.text)\n",
    "                continue\n",
    "            \n",
    "            # Randomly decide whether to replace this word\n",
    "            if random.random() < replacement_prob:\n",
    "                # Only replace nouns, adjectives, verbs, and adverbs\n",
    "                if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "                    replacement = find_similar_word_with_form(token.text, token, w2v_model)\n",
    "                    if replacement:\n",
    "                        new_tokens.append(replacement)\n",
    "                    else:\n",
    "                        new_tokens.append(token.text)\n",
    "                else:\n",
    "                    new_tokens.append(token.text)\n",
    "            else:\n",
    "                new_tokens.append(token.text)\n",
    "        \n",
    "        # Reconstruct text preserving spaces\n",
    "        augmented = \"\"\n",
    "        for i, token in enumerate(doc):\n",
    "            if i < len(new_tokens):\n",
    "                augmented += new_tokens[i]\n",
    "                if i < len(doc) - 1 and not doc[i + 1].is_punct:\n",
    "                    augmented += \" \"\n",
    "        \n",
    "        augmented = augmented.strip()\n",
    "        \n",
    "        if augmented and augmented != text:\n",
    "            variants.append(augmented)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "# Test Word2Vec augmentation\n",
    "print(\"\\nOriginal review:\")\n",
    "print(f\"Label: {'GOOD' if sample_label == 1 else 'BAD'}\")\n",
    "print(f\"Text: {sample_text}\\n\")\n",
    "\n",
    "print(\"Word2Vec augmentations:\")\n",
    "w2v_variants = word2vec_augmentation(sample_text, w2v_model, nlp, num_variants=3)\n",
    "for i, variant in enumerate(w2v_variants, 1):\n",
    "    print(f\"{i}. {variant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "468b1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_augmentation_univ(text, label, num_variants=3):\n",
    "    return word2vec_augmentation(text, w2v_model, nlp, num_variants=num_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9103d",
   "metadata": {},
   "source": [
    "## Test augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a95145b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3(augmentation):\n",
    "    import random\n",
    "    from tqdm.auto import tqdm\n",
    "    lines = open('reviews_for_task3.txt').readlines()\n",
    "\n",
    "    def representation(L):\n",
    "        txt = ' '.join(L)\n",
    "        input_ids = tokenizer(txt, return_tensors='pt')['input_ids'].to(device)\n",
    "        output = model(input_ids=input_ids)\n",
    "        return output.last_hidden_state.detach().cpu().numpy()[0,0,:]\n",
    "\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    N = len(lines)\n",
    "    test_size = N // 4\n",
    "    train_size = N - test_size\n",
    "\n",
    "    train_lines = lines[:train_size]\n",
    "    test_lines = lines[train_size:]\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for line in tqdm(train_lines, desc=\"Processing training set\"):\n",
    "        L = line.split()\n",
    "        y = 0 if L[0] == 'BAD' else 1\n",
    "        \n",
    "        x = representation(L[1:])\n",
    "        y_train.append(y)\n",
    "        X_train.append(x)\n",
    "\n",
    "        for i in range(3):\n",
    "            augs = augmentation(' '.join(L[1:]), y, num_variants=1)\n",
    "            x = representation(augs[0].split())\n",
    "            y_train.append(y)\n",
    "            X_train.append(x)\n",
    "        \n",
    "    for line in tqdm(test_lines, desc=\"Processing test set\"):\n",
    "        L = line.split()\n",
    "        y = 0 if L[0] == 'BAD' else 1\n",
    "        \n",
    "        x = representation(L[1:])\n",
    "        y_test.append(y)\n",
    "        X_test.append(x)\n",
    "            \n",
    "\n",
    "    N = len(lines)\n",
    "    test_size = N // 4\n",
    "    train_size = N - test_size\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import svm \n",
    "\n",
    "    clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "    print ('Train accuracy:', clf.score(X_train, y_train))\n",
    "    print ('Test accuracy:', clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fdd3d354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1759f3b1254b7ab43792095d68cb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training set:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e2a9cfe96744e9ba9378d0d8b57c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9966666666666667\n",
      "Test accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "task3(mechanical_augmentation_univ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2b1cd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c36d2e013a483db1af238a1d47b948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training set:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d44d024979416b86faf2052b93b410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9916666666666667\n",
      "Test accuracy: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "task3(generative_augmentation_univ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8962b988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba98636464474a88929117cec292bb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training set:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-91197072.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_augmentation_univ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-2456042504.py\u001b[0m in \u001b[0;36mtask3\u001b[0;34m(augmentation)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0maugs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_variants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "task3(word2vec_augmentation_univ)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0001ff19dd44411a9323e31d1e39f8f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "00559fa72797476aa1cc4ae81119456e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08a5f4a9285442c3b113268d54c6660f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "095d9af8a7df46e189cf991f96002bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c7f3b5010e94f5a962503a41071f430",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab768c7671d74c02a266f9bbffd2d6f3",
      "value": 288
     }
    },
    "0dc5456d00524f64af7d5d289787f9f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f36098b6b354be4be82e948bae70906",
      "placeholder": "​",
      "style": "IPY_MODEL_0001ff19dd44411a9323e31d1e39f8f1",
      "value": "Generating contextual embeddings: 100%"
     }
    },
    "12baa0ef9ed740399669a409350baa2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1586f4f61130455d8bad84ffa3080bb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f36098b6b354be4be82e948bae70906": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "360f3f750e804ced9a533673cc7091ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39307f6b31594b8d938f539ae5fba3f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aa13ee59fe2443ea6f8f4cf42b0ea66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c7f3b5010e94f5a962503a41071f430": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70db635a2ea94fbf99e8d34c99ce4109": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7250be7228a4473f8c9d73af89312ab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72766ff9a7834618b691a71274dc829c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e999cc245b2a4d8cb472f979a520079b",
       "IPY_MODEL_e78fb6e79f5a4d229f0dc5ef11dd9f71",
       "IPY_MODEL_7592a01796884cfea4f0bca0fe74ddc8"
      ],
      "layout": "IPY_MODEL_825d06bd36de4b6ea2d9b55992e168f2"
     }
    },
    "7592a01796884cfea4f0bca0fe74ddc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_360f3f750e804ced9a533673cc7091ca",
      "placeholder": "​",
      "style": "IPY_MODEL_4aa13ee59fe2443ea6f8f4cf42b0ea66",
      "value": " 288/288 [00:00&lt;00:00, 10442.61it/s]"
     }
    },
    "7a8d6d623e56447ea751c34c9fca6616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "825d06bd36de4b6ea2d9b55992e168f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92a67f5404964244b5a73f97672e178f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ec5fc4082643e690fa55d32ce4a611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a33fb0de5a5242c48eaa63808539b397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7250be7228a4473f8c9d73af89312ab5",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0ec5fc4082643e690fa55d32ce4a611",
      "value": 288
     }
    },
    "a3810f1e846b4993a3d5279d581da879": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00559fa72797476aa1cc4ae81119456e",
      "placeholder": "​",
      "style": "IPY_MODEL_7a8d6d623e56447ea751c34c9fca6616",
      "value": "100%"
     }
    },
    "ab768c7671d74c02a266f9bbffd2d6f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3b7c41c9f9b44e091272ee54750c152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92a67f5404964244b5a73f97672e178f",
      "placeholder": "​",
      "style": "IPY_MODEL_12baa0ef9ed740399669a409350baa2f",
      "value": " 288/288 [00:36&lt;00:00,  7.18it/s]"
     }
    },
    "bebd9dafb5704c02ba4efc47a85c57b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf980457a1e648feb9f806a3788d4280": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4a7778a2b114c22a190d9f5b1f87925",
      "placeholder": "​",
      "style": "IPY_MODEL_bfaab097216f4b7bac4bab49cbf32c73",
      "value": " 288/288 [00:15&lt;00:00, 20.47it/s]"
     }
    },
    "bfaab097216f4b7bac4bab49cbf32c73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4a7778a2b114c22a190d9f5b1f87925": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e612e1f136384991a8b867a67f3cd170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e78fb6e79f5a4d229f0dc5ef11dd9f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1586f4f61130455d8bad84ffa3080bb6",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08a5f4a9285442c3b113268d54c6660f",
      "value": 288
     }
    },
    "e999cc245b2a4d8cb472f979a520079b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bebd9dafb5704c02ba4efc47a85c57b2",
      "placeholder": "​",
      "style": "IPY_MODEL_e612e1f136384991a8b867a67f3cd170",
      "value": "Generating combined embeddings: 100%"
     }
    },
    "f5b94add35fb4c78a74e3070551b4386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a3810f1e846b4993a3d5279d581da879",
       "IPY_MODEL_a33fb0de5a5242c48eaa63808539b397",
       "IPY_MODEL_b3b7c41c9f9b44e091272ee54750c152"
      ],
      "layout": "IPY_MODEL_70db635a2ea94fbf99e8d34c99ce4109"
     }
    },
    "f706829fc019497c830a836f876b798c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dc5456d00524f64af7d5d289787f9f5",
       "IPY_MODEL_095d9af8a7df46e189cf991f96002bd8",
       "IPY_MODEL_bf980457a1e648feb9f806a3788d4280"
      ],
      "layout": "IPY_MODEL_39307f6b31594b8d938f539ae5fba3f2"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
