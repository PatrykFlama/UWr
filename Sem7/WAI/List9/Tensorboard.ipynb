{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorBoard\n",
    "\n",
    "TensorBoard is a visualization and monitoring tool originally developed for TensorFlow, but it is fully compatible with PyTorch through the torch.utils.tensorboard API.\n",
    "It allows you to track and understand what happens during the training of neural networks by providing real-time, interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# split train into train and validation\n",
    "train_size = int(0.9 * len(trainset))\n",
    "valid_size = len(trainset) - train_size\n",
    "\n",
    "train_subset, valid_subset = random_split(trainset, [train_size, valid_size])\n",
    "\n",
    "# dataloaders \n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_subset, batch_size=4,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll set up TensorBoard, importing tensorboard from torch.utils and defining a SummaryWriter, our key object for writing information to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to initialize a SummaryWriter, depending on how you want to structure and label your experiment logs. The examples below illustrate the most common options:\n",
    "\n",
    "```python\n",
    "# create a summary writer with automatically generated folder name.\n",
    "writer = SummaryWriter()\n",
    "# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n",
    "\n",
    "# create a summary writer using the specified folder name.\n",
    "writer = SummaryWriter(\"my_experiment\")\n",
    "# folder location: my_experiment\n",
    "\n",
    "# create a summary writer with comment appended.\n",
    "writer = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n",
    "# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s write an image to our TensorBoard - specifically, a grid - using make_grid.\n",
    "\n",
    "`make_grid` takes several images (a batch) and combines them into a single image arranged in a grid.\n",
    "Internally, it:\n",
    "* takes the tensor (B, C, H, W)\n",
    "* arranges the images into rows and columns\n",
    "* adds optional spacing (padding) between them\n",
    "* returns a single tensor (C, H_grid, W_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running\n",
    "``\n",
    "tensorboard --logdir=runs\n",
    "``\n",
    "from the command line and then navigating to http://localhost:6006 should show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the model using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of TensorBoard’s strengths is its ability to visualize complex model structures. Let’s visualize the model we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and double click on “Net” to see it expand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a “Projector” to TensorBoard\n",
    "\n",
    "We can visualize the lower dimensional representation of higher dimensional data via the `add_embedding` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used to **reduce the dimensionality** of a dataset while preserving as much information (variance) as possible.\n",
    "It transforms a set of possibly correlated variables into a new set of **uncorrelated variables**, called **principal components** (PCs).\n",
    "\n",
    "PCA is useful when working with datasets that have many features, especially when some features are redundant or correlated. It helps with:\n",
    "\n",
    "* visualization of high-dimensional data,\n",
    "* identifying patterns and structure,\n",
    "* removing noise,\n",
    "* reducing model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### How to interpret PCA?\n",
    "\n",
    "After applying PCA, you obtain new axes (PC1, PC2, PC3, …) that represent directions of maximal variance in the data.\n",
    "\n",
    "**Interpretation principles:**\n",
    "\n",
    "#### Principal components represent directions of variance\n",
    "\n",
    "* **PC1** explains the largest amount of variance in the dataset.\n",
    "* **PC2** explains the next largest amount of variance **and is orthogonal to PC1**.\n",
    "* Subsequent PCs capture the remaining variance while remaining orthogonal to all previous components.\n",
    "\n",
    "The first 2–3 components are often enough to show most of the structure in the data.\n",
    "\n",
    "#### Scatter plots of principal components\n",
    "\n",
    "A **PCA plot** (PC1 vs PC2) shows how samples relate to one another in the reduced feature space:\n",
    "\n",
    "* points that cluster together = samples with similar patterns,\n",
    "* outliers appear far from others,\n",
    "* axes represent the strongest patterns in the data.\n",
    "\n",
    "#### Loadings\n",
    "\n",
    "Loadings describe how much each original variable contributes to each principal component.\n",
    "\n",
    "* A large positive or negative loading means that the variable strongly influences the component.\n",
    "* Loadings can be visualized as arrows:\n",
    "\n",
    "  * **direction** shows correlation with the component,\n",
    "  * **length** shows strength (magnitude of loading).\n",
    "\n",
    "The interpretation:\n",
    "**“Variables whose arrows point in a similar direction vary together.”**\n",
    "\n",
    "#### Explained variance\n",
    "\n",
    "Each component has an **explained variance ratio**, telling you how much of the original dataset’s information it retains.\n",
    "\n",
    "* A **scree plot** visualizes this.\n",
    "* Higher explained variance = more important component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "### What is t-SNE?\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) is a **non-linear**, **unsupervised** dimensionality reduction technique used primarily for **visualizing high-dimensional data**.\n",
    "Unlike PCA, which is linear, t-SSE is designed to preserve **local structure** and **small pairwise distances**, making it excellent for exploring complex datasets and revealing clusters.\n",
    "\n",
    "t-SNE is especially useful when:\n",
    "\n",
    "* the data does not follow a linear structure,\n",
    "* there are many intertwined patterns,\n",
    "* we want to visualize data in **2D or 3D** for interpretation.\n",
    "\n",
    "Common applications include NLP embeddings, image feature visualizations, clustering inspection, anomaly detection, and exploratory data analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### How to interpret t-SNE plots\n",
    "\n",
    "t-SNE outputs a 2D or 3D scatter plot where:\n",
    "\n",
    "* **points that appear close** - represent observations similar in the high-dimensional space,\n",
    "* **points far apart** - represent dissimilar observations,\n",
    "* **clusters** often represent meaningful groupings in the original data,\n",
    "* **subclusters** may reveal fine-grained structure.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "* The axes of a t-SNE plot have **no interpretable meaning**.\n",
    "* Distances between clusters can sometimes be distorted; focus mainly on local neighborhoods.\n",
    "* Changing hyperparameters (perplexity, learning rate) may produce different visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n",
    "### What is UMAP?\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) is a **non-linear dimensionality reduction technique** designed to project high-dimensional data into a low-dimensional space (usually 2D or 3D).\n",
    "It is based on principles from **manifold learning** and **topological data analysis**.\n",
    "\n",
    "UMAP is similar to t-SNE but:\n",
    "\n",
    "* preserves **both local and global structure** better,\n",
    "* scales much more efficiently to large datasets,\n",
    "* creates more stable, meaningful embeddings.\n",
    "\n",
    "UMAP is widely used for **visualization, clustering, exploratory data analysis**, and as a **preprocessing step** for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### How to interpret UMAP plots\n",
    "\n",
    "A UMAP plot shows a 2D or 3D embedding of the original high-dimensional data.\n",
    "\n",
    "Basic interpretation rules:\n",
    "\n",
    "* **Points close in the plot** → similar in high-dimensional space.\n",
    "* **Points far apart** → dissimilar high-dimensional patterns.\n",
    "* **Clusters** → groups of observations sharing similar features.\n",
    "* **Cluster shapes and distances** can reflect broader topology (UMAP preserves more global structure than t-SNE).\n",
    "* Axes have **no direct meaning**, similar to t-SNE.\n",
    "\n",
    "UMAP plots often reveal:\n",
    "\n",
    "* distinct clusters,\n",
    "* subgroups within clusters,\n",
    "* trajectories (common in genomics),\n",
    "* outliers and unusual points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking model training with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train the model using the same model training code from the prior tutorial, but writing results to TensorBoard every 1000 batches instead of printing to console; this is done using the `add_scalar` function.\n",
    "\n",
    "It's a good idea to log metrics during training, such as accuracy, F1-score, or loss values. This provides insights into your model's performance over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "One of the key metrics to monitor while training a model is loss, which gets calculated during the forward pass for each batch of data. The diagram below illustrates how loss can change over time and what its pattern reveals about the learning rate:\n",
    "\n",
    "<img src=\"Images/loss.jpeg\" height=\"450\"> | <img src=\"Images/learningrates.jpeg\" height=\"450\">\n",
    "\n",
    "**Left** This shows how different learning rates affect loss. With a low learning rate, improvements tend to be slow and steady, following a linear pattern. On the other hand, a high learning rate speeds up the decay of the loss but often leads to unstable optimization, where the loss doesn’t reach a good minimum because the parameters \"bounce\" chaotically (as seen in the green line).\n",
    "\n",
    "**Right** This graph demonstrates a typical loss curve over time. While this curve seems reasonable, its slow decay might suggest a learning rate that’s too low. Additionally, the noisy fluctuations in the loss could indicate that the batch size is too small.\n",
    "\n",
    "### Log scale\n",
    "<img src=\"Images/log_scale.jpeg\" height=\"450\">\n",
    "\n",
    "When to Use Each Loss Scale?\n",
    "\n",
    "Normal Scale (Linear)\n",
    "* Use when loss values are within a narrow range (e.g., 0.1 to 0.001).\n",
    "* Best for stable, non-exponential loss curves.\n",
    "* Easier to interpret for small loss changes.\n",
    "\n",
    "Log Scale\n",
    "* Use when loss spans multiple orders of magnitude (e.g., 100 → 0.001).\n",
    "* Helps visualize small improvements in later epochs.\n",
    "* Useful when loss decreases exponentially.\n",
    "## Train/Val accuracy\n",
    "Another important metric to track is accuracy, particularly on both the training and validation datasets. This can provide insight into whether the model is overfitting.\n",
    "\n",
    "![Acc](Images/accuracies.jpeg)\n",
    "\n",
    "The plot on the left shows two scenarios. In one case, the validation accuracy is significantly lower than the training accuracy, indicating overfitting. When this happens, you might want to add stronger regularization (like increasing dropout or using an L2 penalty) or collect more training data.\n",
    "\n",
    "In the second scenario, the validation and training accuracies are closely aligned, which suggests the model lacks sufficient complexity. To improve in this case, you should increase the model size by adding more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(loader, net):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    net.train()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 13500/13500 [00:17<00:00, 775.13it/s, loss=0.0149, acc=0.843] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "running_total = 0\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    \n",
    "    pbar = tqdm(enumerate(trainloader, 0), total=len(trainloader))\n",
    "    pbar.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for i, data in pbar:\n",
    "\n",
    "        # inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # accuracy (train)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_total += labels.size(0)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # LIVE UPDATE TQDM \n",
    "        if i % 20 == 0:\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "            train_acc = running_correct / running_total\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_loss:.4f}\",\n",
    "                \"acc\": f\"{train_acc:.3f}\"\n",
    "            })\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            avg_loss = running_loss / 1000\n",
    "            train_acc = running_correct / running_total\n",
    "\n",
    "            # walidacja\n",
    "            val_acc = evaluate_accuracy(validloader, net)\n",
    "\n",
    "            # TensorBoard log: training loss\n",
    "            writer.add_scalar('training loss', avg_loss,\n",
    "                              epoch * len(trainloader) + i)\n",
    "\n",
    "            # TensorBoard: accuracy (train vs val)\n",
    "            writer.add_scalars('accuracy', {\n",
    "                'train': train_acc,\n",
    "                'val': val_acc\n",
    "            }, epoch * len(trainloader) + i)\n",
    "\n",
    "            # reset counters\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            running_total = 0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing trained models with TensorBoard\n",
    "\n",
    "You will now see a “PR Curves” tab that contains the precision-recall curves for each class. Go ahead and poke around; you’ll see that on some classes the model has nearly 100% “area under the curve”, whereas on others this area is lowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-pytorch)",
   "language": "python",
   "name": "nlp-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
