{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sGX66KuOcAb"
      },
      "source": [
        "# Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNWg5dinOcAc"
      },
      "source": [
        "## Introduction to Weights & Biases (W&B)\n",
        "\n",
        "**Weights & Biases (W&B)** is a popular experiment-tracking and model-management platform used in machine learning workflows. It helps you monitor training runs, visualize metrics in real time, compare models, store hyperparameters, and keep your experiments reproducible.\n",
        "\n",
        "W&B integrates seamlessly with deep learning frameworks such as **PyTorch, TensorFlow, Keras, and scikit-learn**, and is widely used in both research and industry. Its goal is to make machine learning experiments easier to track, analyze, and share.\n",
        "\n",
        "### What W&B is used for\n",
        "\n",
        "* **Experiment tracking:** log loss, accuracy, learning rate, gradients, and other metrics during training.\n",
        "* **Hyperparameter management:** store and compare runs with different hyperparameter settings.\n",
        "* **Model comparison:** analyze multiple experiments side-by-side in interactive dashboards.\n",
        "* **Artifact management:** version datasets, models, and trained weights.\n",
        "* **Visualization:** plot metrics, confusion matrices, images, and embeddings.\n",
        "* **Collaboration:** share experiment dashboards with teammates or publish results.\n",
        "\n",
        "\n",
        "## The resulting interactive W&B dashboard will look like:\n",
        "![](https://i.imgur.com/z8TK2Et.png)\n",
        "\n",
        "### In pseudocode, what we'll do is:\n",
        "\n",
        "```python\n",
        "# import the library\n",
        "import wandb\n",
        "\n",
        "# start a new experiment\n",
        "wandb.init(project=\"new-sota-model\")\n",
        "\n",
        "# capture a dictionary of hyperparameters with config\n",
        "wandb.config = {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 128}\n",
        "\n",
        "# set up model and data\n",
        "model, dataloader = get_model(), get_data()\n",
        "\n",
        "# optional: track gradients\n",
        "wandb.watch(model)\n",
        "\n",
        "for batch in dataloader:\n",
        "  metrics = model.training_step()\n",
        "  # log metrics inside your training loop to visualize model performance\n",
        "  wandb.log(metrics)\n",
        "\n",
        "# optional: save model at the end\n",
        "model.to_onnx()\n",
        "wandb.save(\"model.onnx\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOB1HvJgOcAc"
      },
      "source": [
        "# Import, and Log In"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-ihhtfvIOcAe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# remove slow mirror from list of MNIST mirrors\n",
        "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
        "                                      if not mirror.startswith(\"http://yann.lecun.com\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqV8aE-0OcAe"
      },
      "source": [
        "### Import W&B and Login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2L_wNVZOcAe"
      },
      "source": [
        "In order to log data to our web service,\n",
        "you'll need to log in.\n",
        "\n",
        "If this is your first time using W&B,\n",
        "you'll need to sign up for a free account at the link that appears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install wandb onnx -Uq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ak81zyIsOcAf"
      },
      "outputs": [],
      "source": [
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DR4VJ_iDOcAf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaria-szlasa\u001b[0m (\u001b[33mmaria-szlasa-university-of-wroclaw\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_3pkw2tOcAf"
      },
      "source": [
        "# Define the Experiment and Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551KHdy2OcAf"
      },
      "source": [
        "## Track metadata and hyperparameters with `wandb.init`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOuhwO5pOcAf"
      },
      "source": [
        "When setting up an experiment in code, the first step is to clearly define it:\n",
        "which hyperparameters are we using, and what metadata describes this particular run?\n",
        "\n",
        "A common practice is to store all of this information inside a **`config` dictionary** (or a similar structure). This makes it easy to access and reuse parameters throughout the training process.\n",
        "\n",
        "In our example, we only allow a few hyperparameters to change and manually specify the rest. However, **anything in your model — layers, optimizer settings, data options — can be included in the `config`**.\n",
        "\n",
        "We also attach some basic metadata, such as noting that this run uses the MNIST dataset and a convolutional neural network. Later, if we experiment with fully connected models within the same project, this metadata helps us keep different runs organized and easy to distinguish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5AUFcdUjOcAf"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    epochs=5,\n",
        "    classes=10,\n",
        "    kernels=[16, 32],\n",
        "    batch_size=128,\n",
        "    learning_rate=0.005,\n",
        "    dataset=\"MNIST\",\n",
        "    architecture=\"CNN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10QPgXbOcAg"
      },
      "source": [
        "Now, let's define the overall pipeline,\n",
        "which is pretty typical for model-training:\n",
        "\n",
        "1. we first **make a model**, plus associated data and optimizer, then\n",
        "2. we **train the model** accordingly and finally\n",
        "3. **test** it to see how training went.\n",
        "\n",
        "We'll implement these functions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W_LMjHUdOcAg"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"pytorch-demo\", config=hyperparameters):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # and test its final performance\n",
        "      test(model, test_loader)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMrW_gMdOcAg"
      },
      "source": [
        "The only real change from a typical pipeline is that everything happens inside a `wandb.init` block.\n",
        "Calling this function opens a communication channel between your script and the W&B servers.\n",
        "\n",
        "When you pass a `config` dictionary into `wandb.init`, all of those settings are logged right away, so you always have a record of the hyperparameters your experiment used.\n",
        "\n",
        "To make sure the parameters you log are exactly the ones your model relies on, it’s best to use the `wandb.config` version of your configuration object.\n",
        "Take a look at the `make` function definition below for examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0gbyqnY6OcAg"
      },
      "outputs": [],
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train, test = get_data(train=True), get_data(train=False)\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = ConvNet(config.kernels, config.classes).to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA0qTUSbOcAg"
      },
      "source": [
        "# Define the Data Loading and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-zlhXZ8OcAg"
      },
      "source": [
        "Now we just have to define how the data will be loaded and how the model will be structured.\n",
        "\n",
        "This step is crucial, but it’s exactly the same as it would be without using wandb, so we won’t spend extra time on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a7y8a9ZtOcAg"
      },
      "outputs": [],
      "source": [
        "def get_data(slice=5, train=True):\n",
        "    full_dataset = torchvision.datasets.MNIST(root=\".\",\n",
        "                                              train=train,\n",
        "                                              transform=transforms.ToTensor(),\n",
        "                                              download=True)\n",
        "    #  equiv to slicing with [::slice]\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "      full_dataset, indices=range(0, len(full_dataset), slice))\n",
        "\n",
        "    return sub_dataset\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iksdRJfXOcAh"
      },
      "source": [
        "Defining the model is usually the enjoyable part!\n",
        "\n",
        "But since `wandb` doesn’t alter anything here, we’ll just use a standard ConvNet architecture.\n",
        "\n",
        "Feel free to play with the design and run your own experiments — all your results will be recorded on [wandb.ai](https://wandb.ai)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9OhY8aB8OcAh"
      },
      "outputs": [],
      "source": [
        "# Conventional and convolutional neural network\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, kernels, classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWf4Cn2YOcAh"
      },
      "source": [
        "# Define Training Logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0xmMgDXOcAh"
      },
      "source": [
        "Moving on in our `model_pipeline`, it's time to specify how we `train`.\n",
        "\n",
        "Two `wandb` functions come into play here: `watch` and `log`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y8cApT7OcAh"
      },
      "source": [
        "### Track gradients with `wandb.watch` and everything else with `wandb.log`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt7ijSLcOcAh"
      },
      "source": [
        "`wandb.watch` will log the gradients and the parameters of your model,\n",
        "every `log_freq` steps of training.\n",
        "\n",
        "All you need to do is call it before you start training.\n",
        "\n",
        "The rest of the training code remains the same:\n",
        "we iterate over epochs and batches,\n",
        "running forward and backward passes\n",
        "and applying our `optimizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnb3y8pkOcAh"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, criterion, optimizer, config):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(loader) * config.epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for _, (images, labels) in enumerate(loader):\n",
        "\n",
        "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct +=  len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqtXBcAYOcAi"
      },
      "source": [
        "The only difference is in the logging code:\n",
        "where previously you might have reported metrics by printing to the terminal,\n",
        "now you pass the same information to `wandb.log`.\n",
        "\n",
        "`wandb.log` expects a dictionary with strings as keys.\n",
        "These strings identify the objects being logged, which make up the values.\n",
        "You can also optionally log which `step` of training you're on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CssHT2WrOcAi"
      },
      "outputs": [],
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V62SCE_OcAi"
      },
      "source": [
        "# Define Testing Logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tli00V3XOcAi"
      },
      "source": [
        "Once the model is done training, we want to test it:\n",
        "run it against some fresh data from production, perhaps,\n",
        "or apply it to some hand-curated \"hard examples\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Call `wandb.save`\n",
        "\n",
        "This is also a great time to save the model's architecture\n",
        "and final parameters to disk.\n",
        "For maximum compatibility, we'll `export` our model in the\n",
        "[Open Neural Network eXchange (ONNX) format](https://onnx.ai/).\n",
        "\n",
        "Passing that filename to `wandb.save` ensures that the model parameters\n",
        "are saved to W&B's servers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H5MGdrmjOcAj"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {correct / total:%}\")\n",
        "\n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    torch.onnx.export(model, images, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0IoxNYcOcAj"
      },
      "source": [
        "# Run training and watch your metrics live on wandb.ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn9XNQxcOcAj"
      },
      "source": [
        "Now that we've defined the whole pipeline and slipped in\n",
        "those few lines of W&B code,\n",
        "we're ready to run our fully-tracked experiment.\n",
        "\n",
        "We'll report a few links to you:\n",
        "our documentation,\n",
        "the Project page, which organizes all the runs in a project, and\n",
        "the Run page, where this run's results will be stored.\n",
        "\n",
        "Navigate to the Run page and check out these tabs:\n",
        "\n",
        "1. **Charts**, where the model gradients, parameter values, and loss are logged throughout training\n",
        "2. **System**, which contains a variety of system metrics, including Disk I/O utilization, CPU and GPU metrics (watch that temperature soar), and more\n",
        "3. **Logs**, which has a copy of anything pushed to standard out during training\n",
        "4. **Files**, where, once training is complete, you can click on the `model.onnx` to view our network with the [Netron model viewer](https://github.com/lutzroeder/netron).\n",
        "\n",
        "Once the run in finished\n",
        "(i.e. the `with wandb.init` block is exited),\n",
        "we'll also print a summary of the results in the cell output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZmzeDT90OcAj"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mariaszlasa/Documents/Doktorat/Zajęcia/Warsztat AI/Zajęcia 9/wandb/run-20251207_093920-6y12rrxd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo/runs/6y12rrxd' target=\"_blank\">radiant-sun-2</a></strong> to <a href='https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo' target=\"_blank\">https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo/runs/6y12rrxd' target=\"_blank\">https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo/runs/6y12rrxd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a209fb3476a041d4966125a33bffc8cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after 03072 examples: 0.343\n",
            "Loss after 06272 examples: 0.160\n",
            "Loss after 09472 examples: 0.189\n",
            "Loss after 12640 examples: 0.173\n",
            "Loss after 15840 examples: 0.200\n",
            "Loss after 19040 examples: 0.051\n",
            "Loss after 22240 examples: 0.082\n",
            "Loss after 25408 examples: 0.185\n",
            "Loss after 28608 examples: 0.030\n",
            "Loss after 31808 examples: 0.073\n",
            "Loss after 35008 examples: 0.113\n",
            "Loss after 38176 examples: 0.044\n",
            "Loss after 41376 examples: 0.030\n",
            "Loss after 44576 examples: 0.012\n",
            "Loss after 47776 examples: 0.032\n",
            "Loss after 50944 examples: 0.185\n",
            "Loss after 54144 examples: 0.029\n",
            "Loss after 57344 examples: 0.048\n",
            "Accuracy of the model on the 2000 test images: 98.200000%\n",
            "[torch.onnx] Obtain model graph for `ConvNet([...]` with `torch.export.export(..., strict=False)`...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.onnx] Obtain model graph for `ConvNet([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▃▃▃▃▅▅▅▅▆▆▆▆███</td></tr><tr><td>loss</td><td>█▄▅▄▅▂▂▅▁▂▃▂▁▁▁▅▁▂</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.04767</td></tr><tr><td>test_accuracy</td><td>0.982</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">radiant-sun-2</strong> at: <a href='https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo/runs/6y12rrxd' target=\"_blank\">https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo/runs/6y12rrxd</a><br> View project at: <a href='https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo' target=\"_blank\">https://wandb.ai/maria-szlasa-university-of-wroclaw/pytorch-demo</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251207_093920-6y12rrxd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (nlp-pytorch)",
      "language": "python",
      "name": "nlp-pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
